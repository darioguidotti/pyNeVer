<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>pynever.strategies.training API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pynever.strategies.training</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import abc
import pynever.networks as networks
import pynever.datasets as datasets
import pynever.strategies.conversion as cv
import os
import shutil
import torch
import math
import numpy as np
import torch.nn as nn
import torch.nn.functional as funct
import pynever.utilities as utilities


class TrainingStrategy(abc.ABC):
    &#34;&#34;&#34;
    An abstract class used to represent a Training Strategy.

    Methods
    ----------
    train(NeuralNetwork, Dataset)
        Train the neural network of interest using a training strategy determined in the concrete children.

    &#34;&#34;&#34;

    @abc.abstractmethod
    def train(self, network: networks.NeuralNetwork, dataset: datasets.Dataset) -&gt; networks.NeuralNetwork:
        &#34;&#34;&#34;
        Train the neural network of interest using a pruning strategy determined in the concrete children.

        Parameters
        ----------
        network : NeuralNetwork
            The neural network to train.
        dataset : Dataset
            The dataset to use to train the neural network.

        Returns
        ----------
        NeuralNetwork
            The Neural Network resulting from the training of the original network using the training strategy and the
            dataset.

        &#34;&#34;&#34;
        pass


class AdamTraining(TrainingStrategy):
    &#34;&#34;&#34;
    A concrete class used to represent the Adam training strategy.
    This kind of training is based on an Adam optimizer.
    We refer to https://arxiv.org/abs/1412.6980 for theoretical details on the optimization algorithm.

    Attributes
    ----------
    n_epochs : int
        Number of epochs for the training procedure.
    train_batch_size : int
        Dimension for the train batch size for the training procedure
    test_batch_size : int
        Dimension for the test batch size for the training procedure
    learning_rate : float
        Learning rate parameter for the fine tuning procedure.
    betas : Tuple (float, float), optional
        Coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999)).
    eps : float, optional
        Term added to the denominator to improve numerical stability (default: 1e-8).
    weight_decay : float, optional
        Coefficient of the L2 norm regularizer of the Adam optimizer (default: 0).
    cuda : bool, optional
        Whether to use the cuda library for the procedure (default: False).
    train_patience : int, optional
        The number of epochs in which the loss may not decrease before the
        training procedure is interrupted (default: 10).
    scheduler_patience : int, optional
        The number of epochs in which the loss may not decrease before the
        scheduler decrease the learning rate (default: 3).
    batchnorm_decay : float, optional
        It is the coefficient of the L1 norm regularizer applied only to the weights of the batch
        normalization layers. It is a preparatory parameter for pruning strategies which leverages
        the coefficients of the batch normalization layers. It should be selected considering the
        sparsity rate of the related pruning procedure (default: 0).
    l1_decay: float, optional
        Coefficient of the L1 norm regularizer. It should not be used with the weight_decay regularizer.
        It is also a preparatory parameter for pruning strategies which leverages the near-to-zero value of
        the weights (default: 0).
    fine_tuning : bool, optional
        Whether the training procedure should use the fine tuning routine (i.e., the weight with value = 0 are
        not updated by the optimizer (default: False).

    Methods
    ----------
    train(NeuralNetwork, Dataset)
        Train the neural network of interest using the training strategy Adam Training and the dataset passed as an
        argument.

    &#34;&#34;&#34;

    def __init__(self, n_epochs: int, train_batch_size: int, test_batch_size: int, learning_rate: float,
                 betas: (float, float) = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0, cuda: bool = False,
                 train_patience: int = 10, scheduler_patience: int = 3, batchnorm_decay: float = 0,
                 l1_decay: float = 0, fine_tuning: bool = False):

        self.n_epochs = n_epochs
        self.train_batch_size = train_batch_size
        self.test_batch_size = test_batch_size
        self.learning_rate = learning_rate
        self.betas = betas
        self.eps = eps
        self.weight_decay = weight_decay
        self.cuda = cuda
        self.train_patience = train_patience
        self.scheduler_patience = scheduler_patience
        self.batchnorm_decay = batchnorm_decay
        self.l1_decay = l1_decay
        self.fine_tuning = fine_tuning

    def train(self, network: networks.NeuralNetwork, dataset: datasets.Dataset) -&gt; networks.NeuralNetwork:
        &#34;&#34;&#34;
        Train the neural network of interest using the training strategy SGD Training.

        Parameters
        ----------
        network : NeuralNetwork
            The neural network to train.
        dataset : Dataset
            The dataset to use for the training of the network.

        Returns
        ----------
        NeuralNetwork
            The Neural Network resulting from the application of the training strategy to the original network.

        &#34;&#34;&#34;

        pytorch_converter = cv.PyTorchConverter()
        py_net = pytorch_converter.from_neural_network(network)

        py_net = self.__training(py_net, dataset)

        network.alt_rep_cache.clear()
        network.alt_rep_cache.append(py_net)
        network.up_to_date = False

        return network

    def __training(self, net: cv.PyTorchNetwork, dataset: datasets.Dataset) -&gt; cv.PyTorchNetwork:

        &#34;&#34;&#34;
        Training procedure for the PyTorchNetwork.

        Parameters
        ----------
        net : PyTorchNetwork
            The PyTorchNetwork to train.
        dataset : Dataset
            The dataset to use for the training of the PyTorchNetwork

        Returns
        ----------
        PyTorchNetwork
            The trained PyTorchNetwork.

        &#34;&#34;&#34;

        # If the training should be done with the GPU we set the model to cuda.
        if self.cuda:
            net.pytorch_network.cuda()
        else:
            net.pytorch_network.cpu()

        net.pytorch_network.train()
        net.pytorch_network.float()

        # We define the optimizer and the scheduler with the correct parameters.
        optimizer = torch.optim.Adam(params=net.pytorch_network.parameters(), lr=self.learning_rate, betas=self.betas,
                                     eps=self.eps, weight_decay=self.weight_decay)

        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=self.scheduler_patience)

        start_epoch = 0
        training_set = dataset.get_training_set()

        # If a checkpoint exist, we load the checkpoint of interest
        # checkpoints_path = &#39;checkpoints/&#39; + net.identifier + &#39;.pth.tar&#39;
        # best_model_path = &#39;checkpoints/&#39; + net.identifier + &#39;_best.pth.tar&#39;

        checkpoints_path = net.identifier + &#39;.pth.tar&#39;
        best_model_path = net.identifier + &#39;_best.pth.tar&#39;

        if os.path.isfile(checkpoints_path):

            print(&#34;=&gt; loading checkpoint &#39;{}&#39;&#34;.format(checkpoints_path))
            checkpoint = torch.load(checkpoints_path)
            start_epoch = checkpoint[&#39;epoch&#39;]
            best_prec1 = checkpoint[&#39;best_prec1&#39;]
            net.pytorch_network.load_state_dict(checkpoint[&#39;state_dict&#39;])
            optimizer.load_state_dict(checkpoint[&#39;optimizer&#39;])
            best_val_loss = checkpoint[&#39;best_val_loss&#39;]
            epochs_without_decrease = checkpoint[&#39;no_dec_epochs&#39;]
            print(&#34;=&gt; loaded checkpoint &#39;{}&#39; (epoch {}) Prec1: {:f}&#34;
                  .format(checkpoints_path, checkpoint[&#39;epoch&#39;], best_prec1))

        else:
            print(&#34;=&gt; no checkpoint found at &#39;{}&#39;&#34;.format(checkpoints_path))
            best_val_loss = 999
            epochs_without_decrease = 0

        history_score = np.zeros((self.n_epochs - start_epoch + 1, 3))

        # TRAINING

        best_prec1 = 0
        for epoch in range(start_epoch, self.n_epochs):

            if epochs_without_decrease &gt; self.train_patience:
                break

            # EPOCH TRAINING

            net.pytorch_network.train()
            avg_loss = 0
            train_acc = 0
            batch_idx = 0
            data_idx = 0

            while data_idx &lt; len(training_set[0]):

                if data_idx + self.train_batch_size &gt;= len(training_set[0]):
                    last_data_idx = len(training_set[0])
                else:
                    last_data_idx = data_idx + self.train_batch_size

                data = torch.from_numpy(training_set[0][data_idx:last_data_idx, :])
                target = torch.from_numpy(training_set[1][data_idx:last_data_idx])

                if self.cuda:
                    data, target = data.cuda(), target.cuda()

                data, target = torch.autograd.Variable(data), torch.autograd.Variable(target)
                optimizer.zero_grad()
                output = net.pytorch_network(data)
                loss = funct.cross_entropy(output, target)
                avg_loss += loss.data.item()
                pred = output.data.max(1, keepdim=True)[1]
                train_acc += pred.eq(target.data.view_as(pred)).cpu().sum()
                loss.backward()

                # Pruning oriented training: it regularizes the batch norm coef values in order to identify unimportant
                # channels.
                for m in net.pytorch_network.modules():
                    if isinstance(m, nn.BatchNorm1d):
                        m.weight.grad.data.add_(self.batchnorm_decay * torch.sign(m.weight.data))

                # Pruning oriented training: it regularizes the weights in order to identify the unimportant ones.
                for m in net.pytorch_network.modules():

                    if isinstance(m, nn.Linear):
                        m.weight.grad.data.add_(self.l1_decay * torch.sign(m.weight.data))

                # If the fine_tuning flag is set then we assume that the training is used as fine tuning for a
                # weight pruning procedure, therefore the weight with value = 0 are not updated.
                if self.fine_tuning:

                    for m in net.pytorch_network.modules():

                        if isinstance(m, nn.Linear):
                            weight_copy = m.weight.data.abs().clone()
                            if self.cuda:
                                mask = weight_copy.gt(0).float().cuda()
                            else:
                                mask = weight_copy.gt(0).float()
                            m.weight.grad.data.mul_(mask)

                optimizer.step()
                if batch_idx % 100 == 0:
                    print(&#39;Train Epoch: {} [{}/{} ({:.1f}%)]\tLoss: {:.6f}&#39;.format(
                        epoch, batch_idx * len(data), len(training_set[0]),
                               100. * batch_idx / math.floor(len(training_set[0]) / self.train_batch_size),
                        loss.data.item()))

                data_idx += self.train_batch_size
                batch_idx += 1

            history_score[epoch - start_epoch][0] = avg_loss / float(math.floor(len(training_set[0]) /
                                                                                self.train_batch_size))
            history_score[epoch - start_epoch][1] = train_acc / float(math.floor(len(training_set[0]) /
                                                                                 self.train_batch_size))

            # EPOCH TEST

            prec1, test_loss = utilities.testing(net, dataset, self.test_batch_size, self.cuda)

            if test_loss &lt; best_val_loss:
                epochs_without_decrease = 0
                best_val_loss = test_loss
            else:
                epochs_without_decrease += 1

            if scheduler is not None:
                scheduler.step(test_loss)

            # CHECKPOINT

            history_score[epoch - start_epoch][2] = prec1
            is_best = prec1 &gt; best_prec1
            best_prec1 = max(prec1, best_prec1)

            state = {
                &#39;epoch&#39;: epoch + 1,
                &#39;state_dict&#39;: net.pytorch_network.state_dict(),
                &#39;best_prec1&#39;: best_prec1,
                &#39;optimizer&#39;: optimizer.state_dict(),
                &#39;best_val_loss&#39;: best_val_loss,
                &#39;no_dec_epochs&#39;: epochs_without_decrease,
            }
            torch.save(state, checkpoints_path)
            if is_best:
                shutil.copyfile(checkpoints_path, best_model_path)

        print(&#34;Best accuracy: &#34; + str(best_prec1))
        history_score[-1][0] = best_prec1

        if os.path.isfile(checkpoints_path):
            os.remove(checkpoints_path)

        if os.path.isfile(best_model_path):
            os.remove(best_model_path)

        return net


class AdamTrainingRegression(TrainingStrategy):
    &#34;&#34;&#34;
    A concrete class used to represent the Adam training strategy for regression.
    This kind of training is based on an Adam optimizer.
    We refer to https://arxiv.org/abs/1412.6980 for theoretical details on the optimization algorithm.

    Attributes
    ----------
    n_epochs : int
        Number of epochs for the training procedure.
    train_batch_size : int
        Dimension for the train batch size for the training procedure
    test_batch_size : int
        Dimension for the test batch size for the training procedure
    learning_rate : float
        Learning rate parameter for the fine tuning procedure.
    betas : Tuple (float, float), optional
        Coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999)).
    eps : float, optional
        Term added to the denominator to improve numerical stability (default: 1e-8).
    weight_decay : float, optional
        Coefficient of the L2 norm regularizer of the Adam optimizer (default: 0).
    cuda : bool, optional
        Whether to use the cuda library for the procedure (default: False).
    train_patience : int, optional
        The number of epochs in which the loss may not decrease before the
        training procedure is interrupted (default: 10).
    scheduler_patience : int, optional
        The number of epochs in which the loss may not decrease before the
        scheduler decrease the learning rate (default: 3).

    Methods
    ----------
    train(NeuralNetwork, Dataset)
        Train the neural network of interest using the training strategy Adam Training and the dataset passed as an
        argument.

    &#34;&#34;&#34;

    def __init__(self, n_epochs: int, train_batch_size: int, test_batch_size: int, learning_rate: float,
                 betas: (float, float) = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0, cuda: bool = False,
                 train_patience: int = 10, scheduler_patience: int = 3):

        self.n_epochs = n_epochs
        self.train_batch_size = train_batch_size
        self.test_batch_size = test_batch_size
        self.learning_rate = learning_rate
        self.betas = betas
        self.eps = eps
        self.weight_decay = weight_decay
        self.cuda = cuda
        self.train_patience = train_patience
        self.scheduler_patience = scheduler_patience

    def train(self, network: networks.NeuralNetwork, dataset: datasets.Dataset) -&gt; networks.NeuralNetwork:
        &#34;&#34;&#34;
        Train the neural network of interest using the training strategy SGD Training.

        Parameters
        ----------
        network : NeuralNetwork
            The neural network to train.
        dataset : Dataset
            The dataset to use for the training of the network.

        Returns
        ----------
        NeuralNetwork
            The Neural Network resulting from the application of the training strategy to the original network.

        &#34;&#34;&#34;

        pytorch_converter = cv.PyTorchConverter()
        py_net = pytorch_converter.from_neural_network(network)

        py_net = self.__training(py_net, dataset)

        network.alt_rep_cache.clear()
        network.alt_rep_cache.append(py_net)
        network.up_to_date = False

        return network

    def __training(self, net: cv.PyTorchNetwork, dataset: datasets.Dataset) -&gt; cv.PyTorchNetwork:

        &#34;&#34;&#34;
        Training procedure for the PyTorchNetwork.

        Parameters
        ----------
        net : PyTorchNetwork
            The PyTorchNetwork to train.
        dataset : Dataset
            The dataset to use for the training of the PyTorchNetwork

        Returns
        ----------
        PyTorchNetwork
            The trained PyTorchNetwork.

        &#34;&#34;&#34;

        # If the training should be done with the GPU we set the model to cuda.
        if self.cuda:
            net.pytorch_network.cuda()
        else:
            net.pytorch_network.cpu()

        net.pytorch_network.train()
        net.pytorch_network.double()

        # We define the optimizer and the scheduler with the correct parameters.
        optimizer = torch.optim.Adam(params=net.pytorch_network.parameters(), lr=self.learning_rate, betas=self.betas,
                                     eps=self.eps, weight_decay=self.weight_decay)

        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=self.scheduler_patience)

        start_epoch = 0
        training_set = dataset.get_training_set()

        # If a checkpoint exist, we load the checkpoint of interest
        # checkpoints_path = &#39;checkpoints/&#39; + net.identifier + &#39;.pth.tar&#39;
        # best_model_path = &#39;checkpoints/&#39; + net.identifier + &#39;_best.pth.tar&#39;

        checkpoints_path = net.identifier + &#39;.pth.tar&#39;
        best_model_path = net.identifier + &#39;_best.pth.tar&#39;

        if os.path.isfile(checkpoints_path):

            print(&#34;=&gt; loading checkpoint &#39;{}&#39;&#34;.format(checkpoints_path))
            checkpoint = torch.load(checkpoints_path)
            start_epoch = checkpoint[&#39;epoch&#39;]
            best_prec1 = checkpoint[&#39;best_prec1&#39;]
            net.pytorch_network.load_state_dict(checkpoint[&#39;state_dict&#39;])
            optimizer.load_state_dict(checkpoint[&#39;optimizer&#39;])
            best_val_loss = checkpoint[&#39;best_val_loss&#39;]
            epochs_without_decrease = checkpoint[&#39;no_dec_epochs&#39;]
            print(&#34;=&gt; loaded checkpoint &#39;{}&#39; (epoch {}) Prec1: {:f}&#34;
                  .format(checkpoints_path, checkpoint[&#39;epoch&#39;], best_prec1))

        else:
            print(&#34;=&gt; no checkpoint found at &#39;{}&#39;&#34;.format(checkpoints_path))
            best_val_loss = 999
            epochs_without_decrease = 0

        history_score = np.zeros((self.n_epochs - start_epoch + 1, 3))

        # TRAINING

        best_prec1 = 999
        for epoch in range(start_epoch, self.n_epochs):

            if epochs_without_decrease &gt; self.train_patience:
                break

            # EPOCH TRAINING

            net.pytorch_network.train()
            avg_loss = 0
            train_acc = 0
            batch_idx = 0
            data_idx = 0

            while data_idx &lt; len(training_set[0]):

                if data_idx + self.train_batch_size &gt;= len(training_set[0]):
                    last_data_idx = len(training_set[0])
                else:
                    last_data_idx = data_idx + self.train_batch_size

                data = torch.from_numpy(training_set[0][data_idx:last_data_idx, :])
                target = torch.from_numpy(training_set[1][data_idx:last_data_idx])

                if self.cuda:
                    data, target = data.cuda(), target.cuda()

                data, target = torch.autograd.Variable(data), torch.autograd.Variable(target)
                data = data.double()
                target = target.double()
                optimizer.zero_grad()
                output = net.pytorch_network(data)
                loss = funct.mse_loss(output, target)
                avg_loss += loss.data.item()
                loss.backward()

                optimizer.step()
                if batch_idx % 100 == 0:
                    print(&#39;Train Epoch: {} [{}/{} ({:.1f}%)]\tLoss: {:.6f}&#39;.format(
                        epoch, batch_idx * len(data), len(training_set[0]),
                               100. * batch_idx / math.floor(len(training_set[0]) / self.train_batch_size),
                        loss.data.item()))

                data_idx += self.train_batch_size
                batch_idx += 1

            history_score[epoch - start_epoch][0] = avg_loss / float(math.floor(len(training_set[0]) /
                                                                                self.train_batch_size))
            history_score[epoch - start_epoch][1] = train_acc / float(math.floor(len(training_set[0]) /
                                                                                 self.train_batch_size))

            # EPOCH TEST

            test_set = dataset.get_test_set()

            net.pytorch_network.eval()
            net.pytorch_network.double()
            test_loss = 0
            with torch.no_grad():

                batch_idx = 0
                data_idx = 0

                while data_idx &lt; len(test_set[0]):

                    if data_idx + self.test_batch_size &gt;= len(test_set[0]):
                        last_data_idx = len(test_set[0])
                    else:
                        last_data_idx = data_idx + self.test_batch_size

                    data = torch.from_numpy(test_set[0][data_idx:last_data_idx, :])
                    target = torch.from_numpy(test_set[1][data_idx:last_data_idx])

                    if self.cuda:
                        data, target = data.cuda(), target.cuda()

                    data, target = torch.autograd.Variable(data), torch.autograd.Variable(target)
                    data = data.double()
                    target = target.double()
                    output = net.pytorch_network(data)
                    test_loss += funct.mse_loss(output, target).data.item()  # sum up batch loss
                    batch_idx += 1
                    data_idx += self.test_batch_size

            test_loss /= float(math.floor(len(test_set[0]) / self.test_batch_size))
            print(&#39;\nTest set: Average loss: {:.4f}\n&#39;.format(test_loss))

            if test_loss &lt; best_val_loss:
                epochs_without_decrease = 0
                best_val_loss = test_loss
            else:
                epochs_without_decrease += 1

            if scheduler is not None:
                scheduler.step(test_loss)

            # CHECKPOINT

            history_score[epoch - start_epoch][2] = test_loss
            is_best = test_loss &lt; best_prec1
            best_prec1 = min(test_loss, best_prec1)

            state = {
                &#39;epoch&#39;: epoch + 1,
                &#39;state_dict&#39;: net.pytorch_network.state_dict(),
                &#39;best_prec1&#39;: best_prec1,
                &#39;optimizer&#39;: optimizer.state_dict(),
                &#39;best_val_loss&#39;: best_val_loss,
                &#39;no_dec_epochs&#39;: epochs_without_decrease,
            }

            torch.save(state, checkpoints_path)
            if is_best:
                shutil.copyfile(checkpoints_path, best_model_path)

        print(&#34;Best Loss: &#34; + str(best_prec1))
        history_score[-1][0] = best_prec1

        if os.path.isfile(checkpoints_path):
            os.remove(checkpoints_path)

        if os.path.isfile(best_model_path):
            os.remove(best_model_path)

        return net</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="pynever.strategies.training.AdamTraining"><code class="flex name class">
<span>class <span class="ident">AdamTraining</span></span>
<span>(</span><span>n_epochs: int, train_batch_size: int, test_batch_size: int, learning_rate: float, betas: (<class 'float'>, <class 'float'>) = (0.9, 0.999), eps: float = 1e-08, weight_decay: float = 0, cuda: bool = False, train_patience: int = 10, scheduler_patience: int = 3, batchnorm_decay: float = 0, l1_decay: float = 0, fine_tuning: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>A concrete class used to represent the Adam training strategy.
This kind of training is based on an Adam optimizer.
We refer to <a href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</a> for theoretical details on the optimization algorithm.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>n_epochs</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of epochs for the training procedure.</dd>
<dt><strong><code>train_batch_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Dimension for the train batch size for the training procedure</dd>
<dt><strong><code>test_batch_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Dimension for the test batch size for the training procedure</dd>
<dt><strong><code>learning_rate</code></strong> :&ensp;<code>float</code></dt>
<dd>Learning rate parameter for the fine tuning procedure.</dd>
<dt><strong><code>betas</code></strong> :&ensp;<code>Tuple (float, float)</code>, optional</dt>
<dd>Coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999)).</dd>
<dt><strong><code>eps</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Term added to the denominator to improve numerical stability (default: 1e-8).</dd>
<dt><strong><code>weight_decay</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Coefficient of the L2 norm regularizer of the Adam optimizer (default: 0).</dd>
<dt><strong><code>cuda</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to use the cuda library for the procedure (default: False).</dd>
<dt><strong><code>train_patience</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The number of epochs in which the loss may not decrease before the
training procedure is interrupted (default: 10).</dd>
<dt><strong><code>scheduler_patience</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The number of epochs in which the loss may not decrease before the
scheduler decrease the learning rate (default: 3).</dd>
<dt><strong><code>batchnorm_decay</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>It is the coefficient of the L1 norm regularizer applied only to the weights of the batch
normalization layers. It is a preparatory parameter for pruning strategies which leverages
the coefficients of the batch normalization layers. It should be selected considering the
sparsity rate of the related pruning procedure (default: 0).</dd>
<dt><strong><code>l1_decay</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Coefficient of the L1 norm regularizer. It should not be used with the weight_decay regularizer.
It is also a preparatory parameter for pruning strategies which leverages the near-to-zero value of
the weights (default: 0).</dd>
<dt><strong><code>fine_tuning</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether the training procedure should use the fine tuning routine (i.e., the weight with value = 0 are
not updated by the optimizer (default: False).</dd>
</dl>
<h2 id="methods">Methods</h2>
<p>train(NeuralNetwork, Dataset)
Train the neural network of interest using the training strategy Adam Training and the dataset passed as an
argument.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AdamTraining(TrainingStrategy):
    &#34;&#34;&#34;
    A concrete class used to represent the Adam training strategy.
    This kind of training is based on an Adam optimizer.
    We refer to https://arxiv.org/abs/1412.6980 for theoretical details on the optimization algorithm.

    Attributes
    ----------
    n_epochs : int
        Number of epochs for the training procedure.
    train_batch_size : int
        Dimension for the train batch size for the training procedure
    test_batch_size : int
        Dimension for the test batch size for the training procedure
    learning_rate : float
        Learning rate parameter for the fine tuning procedure.
    betas : Tuple (float, float), optional
        Coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999)).
    eps : float, optional
        Term added to the denominator to improve numerical stability (default: 1e-8).
    weight_decay : float, optional
        Coefficient of the L2 norm regularizer of the Adam optimizer (default: 0).
    cuda : bool, optional
        Whether to use the cuda library for the procedure (default: False).
    train_patience : int, optional
        The number of epochs in which the loss may not decrease before the
        training procedure is interrupted (default: 10).
    scheduler_patience : int, optional
        The number of epochs in which the loss may not decrease before the
        scheduler decrease the learning rate (default: 3).
    batchnorm_decay : float, optional
        It is the coefficient of the L1 norm regularizer applied only to the weights of the batch
        normalization layers. It is a preparatory parameter for pruning strategies which leverages
        the coefficients of the batch normalization layers. It should be selected considering the
        sparsity rate of the related pruning procedure (default: 0).
    l1_decay: float, optional
        Coefficient of the L1 norm regularizer. It should not be used with the weight_decay regularizer.
        It is also a preparatory parameter for pruning strategies which leverages the near-to-zero value of
        the weights (default: 0).
    fine_tuning : bool, optional
        Whether the training procedure should use the fine tuning routine (i.e., the weight with value = 0 are
        not updated by the optimizer (default: False).

    Methods
    ----------
    train(NeuralNetwork, Dataset)
        Train the neural network of interest using the training strategy Adam Training and the dataset passed as an
        argument.

    &#34;&#34;&#34;

    def __init__(self, n_epochs: int, train_batch_size: int, test_batch_size: int, learning_rate: float,
                 betas: (float, float) = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0, cuda: bool = False,
                 train_patience: int = 10, scheduler_patience: int = 3, batchnorm_decay: float = 0,
                 l1_decay: float = 0, fine_tuning: bool = False):

        self.n_epochs = n_epochs
        self.train_batch_size = train_batch_size
        self.test_batch_size = test_batch_size
        self.learning_rate = learning_rate
        self.betas = betas
        self.eps = eps
        self.weight_decay = weight_decay
        self.cuda = cuda
        self.train_patience = train_patience
        self.scheduler_patience = scheduler_patience
        self.batchnorm_decay = batchnorm_decay
        self.l1_decay = l1_decay
        self.fine_tuning = fine_tuning

    def train(self, network: networks.NeuralNetwork, dataset: datasets.Dataset) -&gt; networks.NeuralNetwork:
        &#34;&#34;&#34;
        Train the neural network of interest using the training strategy SGD Training.

        Parameters
        ----------
        network : NeuralNetwork
            The neural network to train.
        dataset : Dataset
            The dataset to use for the training of the network.

        Returns
        ----------
        NeuralNetwork
            The Neural Network resulting from the application of the training strategy to the original network.

        &#34;&#34;&#34;

        pytorch_converter = cv.PyTorchConverter()
        py_net = pytorch_converter.from_neural_network(network)

        py_net = self.__training(py_net, dataset)

        network.alt_rep_cache.clear()
        network.alt_rep_cache.append(py_net)
        network.up_to_date = False

        return network

    def __training(self, net: cv.PyTorchNetwork, dataset: datasets.Dataset) -&gt; cv.PyTorchNetwork:

        &#34;&#34;&#34;
        Training procedure for the PyTorchNetwork.

        Parameters
        ----------
        net : PyTorchNetwork
            The PyTorchNetwork to train.
        dataset : Dataset
            The dataset to use for the training of the PyTorchNetwork

        Returns
        ----------
        PyTorchNetwork
            The trained PyTorchNetwork.

        &#34;&#34;&#34;

        # If the training should be done with the GPU we set the model to cuda.
        if self.cuda:
            net.pytorch_network.cuda()
        else:
            net.pytorch_network.cpu()

        net.pytorch_network.train()
        net.pytorch_network.float()

        # We define the optimizer and the scheduler with the correct parameters.
        optimizer = torch.optim.Adam(params=net.pytorch_network.parameters(), lr=self.learning_rate, betas=self.betas,
                                     eps=self.eps, weight_decay=self.weight_decay)

        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=self.scheduler_patience)

        start_epoch = 0
        training_set = dataset.get_training_set()

        # If a checkpoint exist, we load the checkpoint of interest
        # checkpoints_path = &#39;checkpoints/&#39; + net.identifier + &#39;.pth.tar&#39;
        # best_model_path = &#39;checkpoints/&#39; + net.identifier + &#39;_best.pth.tar&#39;

        checkpoints_path = net.identifier + &#39;.pth.tar&#39;
        best_model_path = net.identifier + &#39;_best.pth.tar&#39;

        if os.path.isfile(checkpoints_path):

            print(&#34;=&gt; loading checkpoint &#39;{}&#39;&#34;.format(checkpoints_path))
            checkpoint = torch.load(checkpoints_path)
            start_epoch = checkpoint[&#39;epoch&#39;]
            best_prec1 = checkpoint[&#39;best_prec1&#39;]
            net.pytorch_network.load_state_dict(checkpoint[&#39;state_dict&#39;])
            optimizer.load_state_dict(checkpoint[&#39;optimizer&#39;])
            best_val_loss = checkpoint[&#39;best_val_loss&#39;]
            epochs_without_decrease = checkpoint[&#39;no_dec_epochs&#39;]
            print(&#34;=&gt; loaded checkpoint &#39;{}&#39; (epoch {}) Prec1: {:f}&#34;
                  .format(checkpoints_path, checkpoint[&#39;epoch&#39;], best_prec1))

        else:
            print(&#34;=&gt; no checkpoint found at &#39;{}&#39;&#34;.format(checkpoints_path))
            best_val_loss = 999
            epochs_without_decrease = 0

        history_score = np.zeros((self.n_epochs - start_epoch + 1, 3))

        # TRAINING

        best_prec1 = 0
        for epoch in range(start_epoch, self.n_epochs):

            if epochs_without_decrease &gt; self.train_patience:
                break

            # EPOCH TRAINING

            net.pytorch_network.train()
            avg_loss = 0
            train_acc = 0
            batch_idx = 0
            data_idx = 0

            while data_idx &lt; len(training_set[0]):

                if data_idx + self.train_batch_size &gt;= len(training_set[0]):
                    last_data_idx = len(training_set[0])
                else:
                    last_data_idx = data_idx + self.train_batch_size

                data = torch.from_numpy(training_set[0][data_idx:last_data_idx, :])
                target = torch.from_numpy(training_set[1][data_idx:last_data_idx])

                if self.cuda:
                    data, target = data.cuda(), target.cuda()

                data, target = torch.autograd.Variable(data), torch.autograd.Variable(target)
                optimizer.zero_grad()
                output = net.pytorch_network(data)
                loss = funct.cross_entropy(output, target)
                avg_loss += loss.data.item()
                pred = output.data.max(1, keepdim=True)[1]
                train_acc += pred.eq(target.data.view_as(pred)).cpu().sum()
                loss.backward()

                # Pruning oriented training: it regularizes the batch norm coef values in order to identify unimportant
                # channels.
                for m in net.pytorch_network.modules():
                    if isinstance(m, nn.BatchNorm1d):
                        m.weight.grad.data.add_(self.batchnorm_decay * torch.sign(m.weight.data))

                # Pruning oriented training: it regularizes the weights in order to identify the unimportant ones.
                for m in net.pytorch_network.modules():

                    if isinstance(m, nn.Linear):
                        m.weight.grad.data.add_(self.l1_decay * torch.sign(m.weight.data))

                # If the fine_tuning flag is set then we assume that the training is used as fine tuning for a
                # weight pruning procedure, therefore the weight with value = 0 are not updated.
                if self.fine_tuning:

                    for m in net.pytorch_network.modules():

                        if isinstance(m, nn.Linear):
                            weight_copy = m.weight.data.abs().clone()
                            if self.cuda:
                                mask = weight_copy.gt(0).float().cuda()
                            else:
                                mask = weight_copy.gt(0).float()
                            m.weight.grad.data.mul_(mask)

                optimizer.step()
                if batch_idx % 100 == 0:
                    print(&#39;Train Epoch: {} [{}/{} ({:.1f}%)]\tLoss: {:.6f}&#39;.format(
                        epoch, batch_idx * len(data), len(training_set[0]),
                               100. * batch_idx / math.floor(len(training_set[0]) / self.train_batch_size),
                        loss.data.item()))

                data_idx += self.train_batch_size
                batch_idx += 1

            history_score[epoch - start_epoch][0] = avg_loss / float(math.floor(len(training_set[0]) /
                                                                                self.train_batch_size))
            history_score[epoch - start_epoch][1] = train_acc / float(math.floor(len(training_set[0]) /
                                                                                 self.train_batch_size))

            # EPOCH TEST

            prec1, test_loss = utilities.testing(net, dataset, self.test_batch_size, self.cuda)

            if test_loss &lt; best_val_loss:
                epochs_without_decrease = 0
                best_val_loss = test_loss
            else:
                epochs_without_decrease += 1

            if scheduler is not None:
                scheduler.step(test_loss)

            # CHECKPOINT

            history_score[epoch - start_epoch][2] = prec1
            is_best = prec1 &gt; best_prec1
            best_prec1 = max(prec1, best_prec1)

            state = {
                &#39;epoch&#39;: epoch + 1,
                &#39;state_dict&#39;: net.pytorch_network.state_dict(),
                &#39;best_prec1&#39;: best_prec1,
                &#39;optimizer&#39;: optimizer.state_dict(),
                &#39;best_val_loss&#39;: best_val_loss,
                &#39;no_dec_epochs&#39;: epochs_without_decrease,
            }
            torch.save(state, checkpoints_path)
            if is_best:
                shutil.copyfile(checkpoints_path, best_model_path)

        print(&#34;Best accuracy: &#34; + str(best_prec1))
        history_score[-1][0] = best_prec1

        if os.path.isfile(checkpoints_path):
            os.remove(checkpoints_path)

        if os.path.isfile(best_model_path):
            os.remove(best_model_path)

        return net</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="pynever.strategies.training.TrainingStrategy" href="#pynever.strategies.training.TrainingStrategy">TrainingStrategy</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="pynever.strategies.training.AdamTraining.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self, network: <a title="pynever.networks.NeuralNetwork" href="../networks.html#pynever.networks.NeuralNetwork">NeuralNetwork</a>, dataset: <a title="pynever.datasets.Dataset" href="../datasets.html#pynever.datasets.Dataset">Dataset</a>) ‑> <a title="pynever.networks.NeuralNetwork" href="../networks.html#pynever.networks.NeuralNetwork">NeuralNetwork</a></span>
</code></dt>
<dd>
<div class="desc"><p>Train the neural network of interest using the training strategy SGD Training.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>network</code></strong> :&ensp;<code>NeuralNetwork</code></dt>
<dd>The neural network to train.</dd>
<dt><strong><code>dataset</code></strong> :&ensp;<code>Dataset</code></dt>
<dd>The dataset to use for the training of the network.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>NeuralNetwork</code></dt>
<dd>The Neural Network resulting from the application of the training strategy to the original network.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train(self, network: networks.NeuralNetwork, dataset: datasets.Dataset) -&gt; networks.NeuralNetwork:
    &#34;&#34;&#34;
    Train the neural network of interest using the training strategy SGD Training.

    Parameters
    ----------
    network : NeuralNetwork
        The neural network to train.
    dataset : Dataset
        The dataset to use for the training of the network.

    Returns
    ----------
    NeuralNetwork
        The Neural Network resulting from the application of the training strategy to the original network.

    &#34;&#34;&#34;

    pytorch_converter = cv.PyTorchConverter()
    py_net = pytorch_converter.from_neural_network(network)

    py_net = self.__training(py_net, dataset)

    network.alt_rep_cache.clear()
    network.alt_rep_cache.append(py_net)
    network.up_to_date = False

    return network</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pynever.strategies.training.AdamTrainingRegression"><code class="flex name class">
<span>class <span class="ident">AdamTrainingRegression</span></span>
<span>(</span><span>n_epochs: int, train_batch_size: int, test_batch_size: int, learning_rate: float, betas: (<class 'float'>, <class 'float'>) = (0.9, 0.999), eps: float = 1e-08, weight_decay: float = 0, cuda: bool = False, train_patience: int = 10, scheduler_patience: int = 3)</span>
</code></dt>
<dd>
<div class="desc"><p>A concrete class used to represent the Adam training strategy for regression.
This kind of training is based on an Adam optimizer.
We refer to <a href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</a> for theoretical details on the optimization algorithm.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>n_epochs</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of epochs for the training procedure.</dd>
<dt><strong><code>train_batch_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Dimension for the train batch size for the training procedure</dd>
<dt><strong><code>test_batch_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Dimension for the test batch size for the training procedure</dd>
<dt><strong><code>learning_rate</code></strong> :&ensp;<code>float</code></dt>
<dd>Learning rate parameter for the fine tuning procedure.</dd>
<dt><strong><code>betas</code></strong> :&ensp;<code>Tuple (float, float)</code>, optional</dt>
<dd>Coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999)).</dd>
<dt><strong><code>eps</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Term added to the denominator to improve numerical stability (default: 1e-8).</dd>
<dt><strong><code>weight_decay</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Coefficient of the L2 norm regularizer of the Adam optimizer (default: 0).</dd>
<dt><strong><code>cuda</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to use the cuda library for the procedure (default: False).</dd>
<dt><strong><code>train_patience</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The number of epochs in which the loss may not decrease before the
training procedure is interrupted (default: 10).</dd>
<dt><strong><code>scheduler_patience</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The number of epochs in which the loss may not decrease before the
scheduler decrease the learning rate (default: 3).</dd>
</dl>
<h2 id="methods">Methods</h2>
<p>train(NeuralNetwork, Dataset)
Train the neural network of interest using the training strategy Adam Training and the dataset passed as an
argument.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AdamTrainingRegression(TrainingStrategy):
    &#34;&#34;&#34;
    A concrete class used to represent the Adam training strategy for regression.
    This kind of training is based on an Adam optimizer.
    We refer to https://arxiv.org/abs/1412.6980 for theoretical details on the optimization algorithm.

    Attributes
    ----------
    n_epochs : int
        Number of epochs for the training procedure.
    train_batch_size : int
        Dimension for the train batch size for the training procedure
    test_batch_size : int
        Dimension for the test batch size for the training procedure
    learning_rate : float
        Learning rate parameter for the fine tuning procedure.
    betas : Tuple (float, float), optional
        Coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999)).
    eps : float, optional
        Term added to the denominator to improve numerical stability (default: 1e-8).
    weight_decay : float, optional
        Coefficient of the L2 norm regularizer of the Adam optimizer (default: 0).
    cuda : bool, optional
        Whether to use the cuda library for the procedure (default: False).
    train_patience : int, optional
        The number of epochs in which the loss may not decrease before the
        training procedure is interrupted (default: 10).
    scheduler_patience : int, optional
        The number of epochs in which the loss may not decrease before the
        scheduler decrease the learning rate (default: 3).

    Methods
    ----------
    train(NeuralNetwork, Dataset)
        Train the neural network of interest using the training strategy Adam Training and the dataset passed as an
        argument.

    &#34;&#34;&#34;

    def __init__(self, n_epochs: int, train_batch_size: int, test_batch_size: int, learning_rate: float,
                 betas: (float, float) = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0, cuda: bool = False,
                 train_patience: int = 10, scheduler_patience: int = 3):

        self.n_epochs = n_epochs
        self.train_batch_size = train_batch_size
        self.test_batch_size = test_batch_size
        self.learning_rate = learning_rate
        self.betas = betas
        self.eps = eps
        self.weight_decay = weight_decay
        self.cuda = cuda
        self.train_patience = train_patience
        self.scheduler_patience = scheduler_patience

    def train(self, network: networks.NeuralNetwork, dataset: datasets.Dataset) -&gt; networks.NeuralNetwork:
        &#34;&#34;&#34;
        Train the neural network of interest using the training strategy SGD Training.

        Parameters
        ----------
        network : NeuralNetwork
            The neural network to train.
        dataset : Dataset
            The dataset to use for the training of the network.

        Returns
        ----------
        NeuralNetwork
            The Neural Network resulting from the application of the training strategy to the original network.

        &#34;&#34;&#34;

        pytorch_converter = cv.PyTorchConverter()
        py_net = pytorch_converter.from_neural_network(network)

        py_net = self.__training(py_net, dataset)

        network.alt_rep_cache.clear()
        network.alt_rep_cache.append(py_net)
        network.up_to_date = False

        return network

    def __training(self, net: cv.PyTorchNetwork, dataset: datasets.Dataset) -&gt; cv.PyTorchNetwork:

        &#34;&#34;&#34;
        Training procedure for the PyTorchNetwork.

        Parameters
        ----------
        net : PyTorchNetwork
            The PyTorchNetwork to train.
        dataset : Dataset
            The dataset to use for the training of the PyTorchNetwork

        Returns
        ----------
        PyTorchNetwork
            The trained PyTorchNetwork.

        &#34;&#34;&#34;

        # If the training should be done with the GPU we set the model to cuda.
        if self.cuda:
            net.pytorch_network.cuda()
        else:
            net.pytorch_network.cpu()

        net.pytorch_network.train()
        net.pytorch_network.double()

        # We define the optimizer and the scheduler with the correct parameters.
        optimizer = torch.optim.Adam(params=net.pytorch_network.parameters(), lr=self.learning_rate, betas=self.betas,
                                     eps=self.eps, weight_decay=self.weight_decay)

        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=self.scheduler_patience)

        start_epoch = 0
        training_set = dataset.get_training_set()

        # If a checkpoint exist, we load the checkpoint of interest
        # checkpoints_path = &#39;checkpoints/&#39; + net.identifier + &#39;.pth.tar&#39;
        # best_model_path = &#39;checkpoints/&#39; + net.identifier + &#39;_best.pth.tar&#39;

        checkpoints_path = net.identifier + &#39;.pth.tar&#39;
        best_model_path = net.identifier + &#39;_best.pth.tar&#39;

        if os.path.isfile(checkpoints_path):

            print(&#34;=&gt; loading checkpoint &#39;{}&#39;&#34;.format(checkpoints_path))
            checkpoint = torch.load(checkpoints_path)
            start_epoch = checkpoint[&#39;epoch&#39;]
            best_prec1 = checkpoint[&#39;best_prec1&#39;]
            net.pytorch_network.load_state_dict(checkpoint[&#39;state_dict&#39;])
            optimizer.load_state_dict(checkpoint[&#39;optimizer&#39;])
            best_val_loss = checkpoint[&#39;best_val_loss&#39;]
            epochs_without_decrease = checkpoint[&#39;no_dec_epochs&#39;]
            print(&#34;=&gt; loaded checkpoint &#39;{}&#39; (epoch {}) Prec1: {:f}&#34;
                  .format(checkpoints_path, checkpoint[&#39;epoch&#39;], best_prec1))

        else:
            print(&#34;=&gt; no checkpoint found at &#39;{}&#39;&#34;.format(checkpoints_path))
            best_val_loss = 999
            epochs_without_decrease = 0

        history_score = np.zeros((self.n_epochs - start_epoch + 1, 3))

        # TRAINING

        best_prec1 = 999
        for epoch in range(start_epoch, self.n_epochs):

            if epochs_without_decrease &gt; self.train_patience:
                break

            # EPOCH TRAINING

            net.pytorch_network.train()
            avg_loss = 0
            train_acc = 0
            batch_idx = 0
            data_idx = 0

            while data_idx &lt; len(training_set[0]):

                if data_idx + self.train_batch_size &gt;= len(training_set[0]):
                    last_data_idx = len(training_set[0])
                else:
                    last_data_idx = data_idx + self.train_batch_size

                data = torch.from_numpy(training_set[0][data_idx:last_data_idx, :])
                target = torch.from_numpy(training_set[1][data_idx:last_data_idx])

                if self.cuda:
                    data, target = data.cuda(), target.cuda()

                data, target = torch.autograd.Variable(data), torch.autograd.Variable(target)
                data = data.double()
                target = target.double()
                optimizer.zero_grad()
                output = net.pytorch_network(data)
                loss = funct.mse_loss(output, target)
                avg_loss += loss.data.item()
                loss.backward()

                optimizer.step()
                if batch_idx % 100 == 0:
                    print(&#39;Train Epoch: {} [{}/{} ({:.1f}%)]\tLoss: {:.6f}&#39;.format(
                        epoch, batch_idx * len(data), len(training_set[0]),
                               100. * batch_idx / math.floor(len(training_set[0]) / self.train_batch_size),
                        loss.data.item()))

                data_idx += self.train_batch_size
                batch_idx += 1

            history_score[epoch - start_epoch][0] = avg_loss / float(math.floor(len(training_set[0]) /
                                                                                self.train_batch_size))
            history_score[epoch - start_epoch][1] = train_acc / float(math.floor(len(training_set[0]) /
                                                                                 self.train_batch_size))

            # EPOCH TEST

            test_set = dataset.get_test_set()

            net.pytorch_network.eval()
            net.pytorch_network.double()
            test_loss = 0
            with torch.no_grad():

                batch_idx = 0
                data_idx = 0

                while data_idx &lt; len(test_set[0]):

                    if data_idx + self.test_batch_size &gt;= len(test_set[0]):
                        last_data_idx = len(test_set[0])
                    else:
                        last_data_idx = data_idx + self.test_batch_size

                    data = torch.from_numpy(test_set[0][data_idx:last_data_idx, :])
                    target = torch.from_numpy(test_set[1][data_idx:last_data_idx])

                    if self.cuda:
                        data, target = data.cuda(), target.cuda()

                    data, target = torch.autograd.Variable(data), torch.autograd.Variable(target)
                    data = data.double()
                    target = target.double()
                    output = net.pytorch_network(data)
                    test_loss += funct.mse_loss(output, target).data.item()  # sum up batch loss
                    batch_idx += 1
                    data_idx += self.test_batch_size

            test_loss /= float(math.floor(len(test_set[0]) / self.test_batch_size))
            print(&#39;\nTest set: Average loss: {:.4f}\n&#39;.format(test_loss))

            if test_loss &lt; best_val_loss:
                epochs_without_decrease = 0
                best_val_loss = test_loss
            else:
                epochs_without_decrease += 1

            if scheduler is not None:
                scheduler.step(test_loss)

            # CHECKPOINT

            history_score[epoch - start_epoch][2] = test_loss
            is_best = test_loss &lt; best_prec1
            best_prec1 = min(test_loss, best_prec1)

            state = {
                &#39;epoch&#39;: epoch + 1,
                &#39;state_dict&#39;: net.pytorch_network.state_dict(),
                &#39;best_prec1&#39;: best_prec1,
                &#39;optimizer&#39;: optimizer.state_dict(),
                &#39;best_val_loss&#39;: best_val_loss,
                &#39;no_dec_epochs&#39;: epochs_without_decrease,
            }

            torch.save(state, checkpoints_path)
            if is_best:
                shutil.copyfile(checkpoints_path, best_model_path)

        print(&#34;Best Loss: &#34; + str(best_prec1))
        history_score[-1][0] = best_prec1

        if os.path.isfile(checkpoints_path):
            os.remove(checkpoints_path)

        if os.path.isfile(best_model_path):
            os.remove(best_model_path)

        return net</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="pynever.strategies.training.TrainingStrategy" href="#pynever.strategies.training.TrainingStrategy">TrainingStrategy</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="pynever.strategies.training.AdamTrainingRegression.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self, network: <a title="pynever.networks.NeuralNetwork" href="../networks.html#pynever.networks.NeuralNetwork">NeuralNetwork</a>, dataset: <a title="pynever.datasets.Dataset" href="../datasets.html#pynever.datasets.Dataset">Dataset</a>) ‑> <a title="pynever.networks.NeuralNetwork" href="../networks.html#pynever.networks.NeuralNetwork">NeuralNetwork</a></span>
</code></dt>
<dd>
<div class="desc"><p>Train the neural network of interest using the training strategy SGD Training.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>network</code></strong> :&ensp;<code>NeuralNetwork</code></dt>
<dd>The neural network to train.</dd>
<dt><strong><code>dataset</code></strong> :&ensp;<code>Dataset</code></dt>
<dd>The dataset to use for the training of the network.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>NeuralNetwork</code></dt>
<dd>The Neural Network resulting from the application of the training strategy to the original network.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train(self, network: networks.NeuralNetwork, dataset: datasets.Dataset) -&gt; networks.NeuralNetwork:
    &#34;&#34;&#34;
    Train the neural network of interest using the training strategy SGD Training.

    Parameters
    ----------
    network : NeuralNetwork
        The neural network to train.
    dataset : Dataset
        The dataset to use for the training of the network.

    Returns
    ----------
    NeuralNetwork
        The Neural Network resulting from the application of the training strategy to the original network.

    &#34;&#34;&#34;

    pytorch_converter = cv.PyTorchConverter()
    py_net = pytorch_converter.from_neural_network(network)

    py_net = self.__training(py_net, dataset)

    network.alt_rep_cache.clear()
    network.alt_rep_cache.append(py_net)
    network.up_to_date = False

    return network</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pynever.strategies.training.TrainingStrategy"><code class="flex name class">
<span>class <span class="ident">TrainingStrategy</span></span>
</code></dt>
<dd>
<div class="desc"><p>An abstract class used to represent a Training Strategy.</p>
<h2 id="methods">Methods</h2>
<p>train(NeuralNetwork, Dataset)
Train the neural network of interest using a training strategy determined in the concrete children.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TrainingStrategy(abc.ABC):
    &#34;&#34;&#34;
    An abstract class used to represent a Training Strategy.

    Methods
    ----------
    train(NeuralNetwork, Dataset)
        Train the neural network of interest using a training strategy determined in the concrete children.

    &#34;&#34;&#34;

    @abc.abstractmethod
    def train(self, network: networks.NeuralNetwork, dataset: datasets.Dataset) -&gt; networks.NeuralNetwork:
        &#34;&#34;&#34;
        Train the neural network of interest using a pruning strategy determined in the concrete children.

        Parameters
        ----------
        network : NeuralNetwork
            The neural network to train.
        dataset : Dataset
            The dataset to use to train the neural network.

        Returns
        ----------
        NeuralNetwork
            The Neural Network resulting from the training of the original network using the training strategy and the
            dataset.

        &#34;&#34;&#34;
        pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="pynever.strategies.training.AdamTraining" href="#pynever.strategies.training.AdamTraining">AdamTraining</a></li>
<li><a title="pynever.strategies.training.AdamTrainingRegression" href="#pynever.strategies.training.AdamTrainingRegression">AdamTrainingRegression</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="pynever.strategies.training.TrainingStrategy.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self, network: <a title="pynever.networks.NeuralNetwork" href="../networks.html#pynever.networks.NeuralNetwork">NeuralNetwork</a>, dataset: <a title="pynever.datasets.Dataset" href="../datasets.html#pynever.datasets.Dataset">Dataset</a>) ‑> <a title="pynever.networks.NeuralNetwork" href="../networks.html#pynever.networks.NeuralNetwork">NeuralNetwork</a></span>
</code></dt>
<dd>
<div class="desc"><p>Train the neural network of interest using a pruning strategy determined in the concrete children.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>network</code></strong> :&ensp;<code>NeuralNetwork</code></dt>
<dd>The neural network to train.</dd>
<dt><strong><code>dataset</code></strong> :&ensp;<code>Dataset</code></dt>
<dd>The dataset to use to train the neural network.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>NeuralNetwork</code></dt>
<dd>The Neural Network resulting from the training of the original network using the training strategy and the
dataset.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abc.abstractmethod
def train(self, network: networks.NeuralNetwork, dataset: datasets.Dataset) -&gt; networks.NeuralNetwork:
    &#34;&#34;&#34;
    Train the neural network of interest using a pruning strategy determined in the concrete children.

    Parameters
    ----------
    network : NeuralNetwork
        The neural network to train.
    dataset : Dataset
        The dataset to use to train the neural network.

    Returns
    ----------
    NeuralNetwork
        The Neural Network resulting from the training of the original network using the training strategy and the
        dataset.

    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pynever.strategies" href="index.html">pynever.strategies</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="pynever.strategies.training.AdamTraining" href="#pynever.strategies.training.AdamTraining">AdamTraining</a></code></h4>
<ul class="">
<li><code><a title="pynever.strategies.training.AdamTraining.train" href="#pynever.strategies.training.AdamTraining.train">train</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pynever.strategies.training.AdamTrainingRegression" href="#pynever.strategies.training.AdamTrainingRegression">AdamTrainingRegression</a></code></h4>
<ul class="">
<li><code><a title="pynever.strategies.training.AdamTrainingRegression.train" href="#pynever.strategies.training.AdamTrainingRegression.train">train</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pynever.strategies.training.TrainingStrategy" href="#pynever.strategies.training.TrainingStrategy">TrainingStrategy</a></code></h4>
<ul class="">
<li><code><a title="pynever.strategies.training.TrainingStrategy.train" href="#pynever.strategies.training.TrainingStrategy.train">train</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>