<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>pynever.strategies.pruning API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pynever.strategies.pruning</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import abc
import pynever.networks as networks
import pynever.datasets as datasets
import pynever.strategies.conversion as cv
import pynever.strategies.training as training
import torch
import math
import torch.nn as nn


class PruningStrategy(abc.ABC):
    &#34;&#34;&#34;
    An abstract class used to represent a Pruning Strategy.

    Methods
    ----------
    prune(NeuralNetwork, Dataset)
        Prune the neural network of interest using a pruning strategy determined in the concrete children.

    &#34;&#34;&#34;

    @abc.abstractmethod
    def prune(self, network: networks.NeuralNetwork, dataset: datasets.Dataset) -&gt; networks.NeuralNetwork:
        &#34;&#34;&#34;
        Prune the neural network of interest using a pruning strategy determined in the concrete children.

        Parameters
        ----------
        network : NeuralNetwork
            The neural network to prune.
        dataset: Dataset
            The dataset to use for the pre-training and fine-tuning procedure.

        Returns
        ----------
        NeuralNetwork
            The Neural Network resulting from the application of the pruning strategy to the original network.

        &#34;&#34;&#34;
        pass


class WeightPruning(PruningStrategy):
    &#34;&#34;&#34;
    A concrete class used to represent the weight pruning strategy.
    This kind of pruning select the least important weights of the neural network
    of interest and set them to 0.
    We refer to https://arxiv.org/abs/1506.02626 for theoretical details on the strategy.

    Attributes
    ----------
    sparsity_rate : float
        It determines the percentage of neurons which will be removed. It must be a Real number between 0 and 1.
    training_strategy : AdamTraining
        The training strategy to use for pre-training and/or fine-tuning.
    pre_training : bool
        Flag to indicate if the network need to be pre-trained.

    Methods
    ----------
    prune(NeuralNetwork, Dataset)
        Prune the neural network of interest using the pruning strategy Weight Pruning.

    &#34;&#34;&#34;

    def __init__(self, sparsity_rate: float, training_strategy: training.AdamTraining = None,
                 pre_training: bool = False):

        self.sparsity_rate = sparsity_rate
        self.training_strategy = training_strategy
        self.pre_training = pre_training

    def prune(self, network: networks.NeuralNetwork, dataset: datasets.Dataset) -&gt; networks.NeuralNetwork:
        &#34;&#34;&#34;
        Prune the neural network of interest using the pruning strategy Weight Pruning.

        Parameters
        ----------
        network : NeuralNetwork
            The neural network to prune.
        dataset : Dataset
            The dataset to use for the pre-training and fine-tuning procedure.

        Returns
        ----------
        NeuralNetwork
            The Neural Network resulting from the application of the pruning strategy to the original network.

        &#34;&#34;&#34;

        if self.training_strategy is not None and self.pre_training:
            fine_tuning = self.training_strategy.fine_tuning
            self.training_strategy.fine_tuning = False
            network = self.training_strategy.train(network, dataset)
            self.training_strategy.fine_tuning = fine_tuning

        pytorch_converter = cv.PyTorchConverter()
        py_net = pytorch_converter.from_neural_network(network)

        py_net = self.__pruning(py_net)

        network.alt_rep_cache.clear()
        network.alt_rep_cache.append(py_net)
        network.up_to_date = False

        if self.training_strategy is not None and self.training_strategy.fine_tuning:
            old_l1_decay = self.training_strategy.l1_decay
            old_batchnorm_decay = self.training_strategy.batchnorm_decay
            self.training_strategy.l1_decay = 0
            self.training_strategy.batchnorm_decay = 0
            network = self.training_strategy.train(network, dataset)
            self.training_strategy.l1_decay = old_l1_decay
            self.training_strategy.batchnorm_decay = old_batchnorm_decay

        return network

    def __pruning(self, net: cv.PyTorchNetwork):
        &#34;&#34;&#34;
        Procedure for the pruning of the weights of the PyTorchNetwork passed as an argument.

        Parameters
        ----------
        net : PyTorchNetwork
            The PyTorchNetwork to prune.

        Returns
        ----------
        PyTorchNetwork
            The pruned PyTorchNetwork.

        &#34;&#34;&#34;

        # We transfer the internal pytorch model to the CPU for the pruning procedure.
        net.pytorch_network.cpu()

        # We compute the number of weights in the network
        num_weights = 0
        for m in net.pytorch_network.modules():

            if isinstance(m, torch.nn.Linear):
                num_weights += m.weight.numel()

        # We copy all the absolute values of the weights in a new tensor and we sort in ascending order
        weights = torch.zeros(num_weights)
        index = 0
        for m in net.pytorch_network.modules():

            if isinstance(m, torch.nn.Linear):
                size = m.weight.numel()
                weights[index:(index + size)] = m.weight.view(-1).abs().clone()
                index += size

        ordered_weights, ordered_indexes = torch.sort(weights)

        # We determine the number of weights we need to set to 0, given the sparsity rate.
        threshold_index = math.floor(num_weights * self.sparsity_rate)

        # We select the weight absolute value we will use as threshold value given the threshold index.
        threshold_value = ordered_weights[threshold_index]

        # We set all the weights of the different layers to 0 if they are less or equal than the threshold value
        # (in absolute value)

        for m in net.pytorch_network.modules():

            if isinstance(m, torch.nn.Linear):
                # The values of the mask are 0 when the corresponding weight is less then the threshold_value (in
                # absolute value), otherwise they are 1
                mask = m.weight.abs().gt(threshold_value).float()
                m.weight.data = torch.mul(m.weight, mask)

        return net


class NetworkSlimming(PruningStrategy):
    &#34;&#34;&#34;
    A concrete class used to represent the network slimming pruning strategy.
    This kind of pruning select the least important neurons of the neural network
    of interest and eliminates them. It needs a batch normalization layer following each layer
    which should be pruned. We assume that the activation function is always applied after the batch
    normalization layer.
    We refer to https://arxiv.org/abs/1708.06519 for theoretical details on the strategy.

    Attributes
    ----------
    sparsity_rate : float
        It determines the percentage of neurons which will be removed. It must be a Real number between 0 and 1.
    training_strategy : AdamTraining
        The training strategy to use for pre-training and/or fine-tuning.
    pre_training : bool
        Flag to indicate if the network need to be pre-trained.

    Methods
    ----------
    prune(NeuralNetwork, Dataset)
        Prune the neural network of interest using the pruning strategy Network Slimming.

    &#34;&#34;&#34;

    def __init__(self, sparsity_rate: float, training_strategy: training.AdamTraining = None,
                 pre_training: bool = False):

        self.sparsity_rate = sparsity_rate
        self.training_strategy = training_strategy
        self.pre_training = pre_training

    def prune(self, network: networks.NeuralNetwork, dataset: datasets.Dataset) -&gt; networks.NeuralNetwork:
        &#34;&#34;&#34;
        Prune the neural network of interest using the pruning strategy Network Slimming.

        Parameters
        ----------
        network : NeuralNetwork
            The neural network to prune.
        dataset: Dataset
            The dataset to use for the pre-training and fine-tuning procedure.

        Returns
        ----------
        NeuralNetwork
            The Neural Network resulting from the application of the pruning strategy to the original network.

        &#34;&#34;&#34;

        if self.training_strategy is not None and self.pre_training:
            fine_tuning = self.training_strategy.fine_tuning
            self.training_strategy.fine_tuning = False
            network = self.training_strategy.train(network, dataset)
            self.training_strategy.fine_tuning = fine_tuning

        pytorch_converter = cv.PyTorchConverter()
        py_net = pytorch_converter.from_neural_network(network)

        py_net = self.__pruning(py_net)

        network.alt_rep_cache.clear()
        network.alt_rep_cache.append(py_net)
        network.up_to_date = False

        if self.training_strategy is not None and self.training_strategy.fine_tuning:
            old_l1_decay = self.training_strategy.l1_decay
            old_batchnorm_decay = self.training_strategy.batchnorm_decay
            self.training_strategy.l1_decay = 0
            self.training_strategy.batchnorm_decay = 0
            network = self.training_strategy.train(network, dataset)
            self.training_strategy.l1_decay = old_l1_decay
            self.training_strategy.batchnorm_decay = old_batchnorm_decay

        return network

    def __pruning(self, net: cv.PyTorchNetwork):
        &#34;&#34;&#34;
        Procedure for the pruning of the neurons of the PyTorchNetwork passed as an argument.

        Parameters
        ----------
        net : PyTorchNetwork
            The PyTorchNetwork to prune.

        Returns
        ----------
        PyTorchNetwork
            The PyTorchNetwork resulting from the application of the pure pruning procedure.

        &#34;&#34;&#34;

        # We transfer the internal pytorch model to the CPU for the pruning procedure.
        net.pytorch_network.cpu()

        # We compute the total number of weights in the batch normalization layers (which, for fully connected networks,
        # is equal to the number of neurons in the corresponding fully-connected layer).
        num_bn_weights = 0
        for m in net.pytorch_network.modules():
            if isinstance(m, torch.nn.BatchNorm1d):
                num_bn_weights += m.weight.numel()

        # We copy all the absolute values of the batch norm weights in a new tensor and we sort in ascending order
        bn_weights = torch.zeros(num_bn_weights)
        bn_weights_index = 0
        for m in net.pytorch_network.modules():
            if isinstance(m, torch.nn.BatchNorm1d):
                size = m.weight.numel()
                bn_weights[bn_weights_index:(bn_weights_index + size)] = m.weight.abs().clone()
                bn_weights_index += size

        ordered_bn_weights, ordered_bn_indexes = torch.sort(bn_weights)

        # We determine the number of neurons we need to remove, given the sparsity rate.
        threshold_index = math.floor(num_bn_weights * self.sparsity_rate)

        # We select the batch norm weight absolute value we will use as threshold value given the threshold index.
        threshold_value = ordered_bn_weights[threshold_index]

        # We now need to create a new network with the correct number of neurons in the different layers.
        # To do so we assume that in the network after a linear layer there is always a batch norm layer.

        new_layers = []
        previous_layer_mask = None
        old_layers = [m for m in net.pytorch_network.modules()]
        num_layers = len(old_layers)
        for i in range(num_layers):

            if (i == num_layers - 1) and isinstance(old_layers[i], torch.nn.Linear):

                # In this case we are considering the last layer of the network (which we assume to be a linear layer),
                # therefore the number of output of the new layer will be equal to the one of the old layer.

                previous_nonzero_indexes = previous_layer_mask.nonzero(as_tuple=True)[0]

                # If the old linear layer had bias then also the new linear layer has them.
                if old_layers[i].bias is None:
                    has_bias = False
                else:
                    has_bias = True

                # The number of input features for the new linear layer is equal to the number of non-zero elements in
                # the mask of the previous layer.
                num_in_features = int(previous_layer_mask.sum().item())

                # We create the new linear layer with the correct architecture.
                print(num_in_features, old_layers[i].out_features, has_bias)
                new_linear_layer = torch.nn.Linear(num_in_features, old_layers[i].out_features, has_bias)

                # We copy the parameters corresponding to the still existing neurons.
                new_linear_layer.weight.data = old_layers[i].weight[:, previous_nonzero_indexes].clone()

                if has_bias:
                    new_linear_layer.bias.data = old_layers[i].bias.data

                # We save the new linear layer.
                new_layers.append(new_linear_layer)

            elif isinstance(old_layers[i], torch.nn.Linear) and isinstance(old_layers[i + 1], torch.nn.BatchNorm1d):

                # If the layer old_layers[i] is the first linear layer then the previous layer mask corrspond to the
                # complete input.
                if previous_layer_mask is None:
                    previous_layer_mask = torch.ones(old_layers[i].in_features)

                # We compute the mask corresponding to the batch normalization layer.
                layer_mask = old_layers[i + 1].weight.abs().gt(threshold_value).float()
                new_neuron_number = int(layer_mask.sum().item())

                # We compute the indexes of the non-zero weights for the current batch norm layer and the previous one.
                current_nonzero_indexes = layer_mask.nonzero(as_tuple=True)[0]
                previous_nonzero_indexes = previous_layer_mask.nonzero(as_tuple=True)[0]

                # We create the new batch norm layer with the new neuron number.

                new_bn_layer = torch.nn.BatchNorm1d(new_neuron_number, eps=old_layers[i + 1].eps,
                                                    momentum=old_layers[i + 1].momentum,
                                                    affine=old_layers[i + 1].affine,
                                                    track_running_stats=old_layers[i + 1].track_running_stats)

                # We copy the parameters corresponding to the still existing neurons from the old batch norm layer
                # to the new one. They are identified by the indexes in current_nonzero_indexes.

                new_bn_layer.weight.data = old_layers[i + 1].weight[current_nonzero_indexes].clone()
                new_bn_layer.bias.data = old_layers[i + 1].bias[current_nonzero_indexes].clone()
                new_bn_layer.running_mean = old_layers[i + 1].running_mean[current_nonzero_indexes].clone()
                new_bn_layer.running_var = old_layers[i + 1].running_var[current_nonzero_indexes].clone()

                # If the old linear layer had bias then also the new linear layer has them.
                if old_layers[i].bias is None:
                    has_bias = False
                else:
                    has_bias = True

                # The number of input features for the new linear layer is equal to the number of non-zero elements in
                # the mask of the previous layer.
                num_in_features = int(previous_layer_mask.sum().item())

                # We create the new linear layer with the correct architecture.
                new_linear_layer = torch.nn.Linear(num_in_features, new_neuron_number, has_bias)

                # We copy the parameters corresponding to the still existing neurons.
                new_linear_layer.weight.data = old_layers[i].weight[current_nonzero_indexes, :].clone()
                new_linear_layer.weight.data = new_linear_layer.weight[:, previous_nonzero_indexes].clone()

                if has_bias:
                    new_linear_layer.bias.data = old_layers[i].bias[current_nonzero_indexes]

                # We save the new layers in the order in which they should be in our sequential model: first the linear
                # layer and then the batch norm layer.
                new_layers.append(new_linear_layer)
                new_layers.append(new_bn_layer)

                # We update the value of previous_layer_mask with the current mask.
                previous_layer_mask = layer_mask

            elif isinstance(old_layers[i], torch.nn.Linear) and not isinstance(old_layers[i + 1], torch.nn.BatchNorm1d):

                # If the layer old_layers[i] is the first linear layer then the previous layer mask correspond to the
                # complete input.
                if previous_layer_mask is None:
                    previous_layer_mask = torch.ones(old_layers[i].in_features)

                # If the linear layer is not followed by a batch normalization layer then it will not be neuron pruned,
                # therefore the layer_mask will be equals to the number of output features of the old layer.
                layer_mask = torch.ones(old_layers[i].out_features)

                # We compute the indexes of the non-zero weights for the current batch norm layer and the previous one.
                current_nonzero_indexes = layer_mask.nonzero(as_tuple=True)[0]
                previous_nonzero_indexes = previous_layer_mask.nonzero(as_tuple=True)[0]

                if old_layers[i].bias is None:
                    has_bias = False
                else:
                    has_bias = True

                # The number of input features for the new linear layer is equal to the number of non-zero elements in
                # the mask of the previous layer.
                num_in_features = previous_layer_mask.sum().item()

                # We create the new linear layer with the correct architecture.
                new_linear_layer = torch.nn.Linear(num_in_features, old_layers[i].out_features, has_bias)

                # We copy the parameters corresponding to the still existing neurons.
                new_linear_layer.weight.data = old_layers[i].weight[current_nonzero_indexes, :].clone()
                new_linear_layer.weight.data = new_linear_layer.weight[:, previous_nonzero_indexes].clone()

                if has_bias:
                    new_linear_layer.bias.data = old_layers[i].bias[current_nonzero_indexes]

                # We save the new layers in the order in which they should be in our sequential model: first the linear
                # layer and then the batch norm layer.
                new_layers.append(new_linear_layer)

            elif isinstance(old_layers[i], nn.ReLU):
                new_layers.append(nn.ReLU())

        pruned_network = torch.nn.Sequential(*new_layers)
        net.pytorch_network = pruned_network
        net.identifier = net.identifier + &#39;_pruned&#39;
        return net</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="pynever.strategies.pruning.NetworkSlimming"><code class="flex name class">
<span>class <span class="ident">NetworkSlimming</span></span>
<span>(</span><span>sparsity_rate: float, training_strategy: <a title="pynever.strategies.training.AdamTraining" href="training.html#pynever.strategies.training.AdamTraining">AdamTraining</a> = None, pre_training: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>A concrete class used to represent the network slimming pruning strategy.
This kind of pruning select the least important neurons of the neural network
of interest and eliminates them. It needs a batch normalization layer following each layer
which should be pruned. We assume that the activation function is always applied after the batch
normalization layer.
We refer to <a href="https://arxiv.org/abs/1708.06519">https://arxiv.org/abs/1708.06519</a> for theoretical details on the strategy.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>sparsity_rate</code></strong> :&ensp;<code>float</code></dt>
<dd>It determines the percentage of neurons which will be removed. It must be a Real number between 0 and 1.</dd>
<dt><strong><code>training_strategy</code></strong> :&ensp;<code>AdamTraining</code></dt>
<dd>The training strategy to use for pre-training and/or fine-tuning.</dd>
<dt><strong><code>pre_training</code></strong> :&ensp;<code>bool</code></dt>
<dd>Flag to indicate if the network need to be pre-trained.</dd>
</dl>
<h2 id="methods">Methods</h2>
<p>prune(NeuralNetwork, Dataset)
Prune the neural network of interest using the pruning strategy Network Slimming.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class NetworkSlimming(PruningStrategy):
    &#34;&#34;&#34;
    A concrete class used to represent the network slimming pruning strategy.
    This kind of pruning select the least important neurons of the neural network
    of interest and eliminates them. It needs a batch normalization layer following each layer
    which should be pruned. We assume that the activation function is always applied after the batch
    normalization layer.
    We refer to https://arxiv.org/abs/1708.06519 for theoretical details on the strategy.

    Attributes
    ----------
    sparsity_rate : float
        It determines the percentage of neurons which will be removed. It must be a Real number between 0 and 1.
    training_strategy : AdamTraining
        The training strategy to use for pre-training and/or fine-tuning.
    pre_training : bool
        Flag to indicate if the network need to be pre-trained.

    Methods
    ----------
    prune(NeuralNetwork, Dataset)
        Prune the neural network of interest using the pruning strategy Network Slimming.

    &#34;&#34;&#34;

    def __init__(self, sparsity_rate: float, training_strategy: training.AdamTraining = None,
                 pre_training: bool = False):

        self.sparsity_rate = sparsity_rate
        self.training_strategy = training_strategy
        self.pre_training = pre_training

    def prune(self, network: networks.NeuralNetwork, dataset: datasets.Dataset) -&gt; networks.NeuralNetwork:
        &#34;&#34;&#34;
        Prune the neural network of interest using the pruning strategy Network Slimming.

        Parameters
        ----------
        network : NeuralNetwork
            The neural network to prune.
        dataset: Dataset
            The dataset to use for the pre-training and fine-tuning procedure.

        Returns
        ----------
        NeuralNetwork
            The Neural Network resulting from the application of the pruning strategy to the original network.

        &#34;&#34;&#34;

        if self.training_strategy is not None and self.pre_training:
            fine_tuning = self.training_strategy.fine_tuning
            self.training_strategy.fine_tuning = False
            network = self.training_strategy.train(network, dataset)
            self.training_strategy.fine_tuning = fine_tuning

        pytorch_converter = cv.PyTorchConverter()
        py_net = pytorch_converter.from_neural_network(network)

        py_net = self.__pruning(py_net)

        network.alt_rep_cache.clear()
        network.alt_rep_cache.append(py_net)
        network.up_to_date = False

        if self.training_strategy is not None and self.training_strategy.fine_tuning:
            old_l1_decay = self.training_strategy.l1_decay
            old_batchnorm_decay = self.training_strategy.batchnorm_decay
            self.training_strategy.l1_decay = 0
            self.training_strategy.batchnorm_decay = 0
            network = self.training_strategy.train(network, dataset)
            self.training_strategy.l1_decay = old_l1_decay
            self.training_strategy.batchnorm_decay = old_batchnorm_decay

        return network

    def __pruning(self, net: cv.PyTorchNetwork):
        &#34;&#34;&#34;
        Procedure for the pruning of the neurons of the PyTorchNetwork passed as an argument.

        Parameters
        ----------
        net : PyTorchNetwork
            The PyTorchNetwork to prune.

        Returns
        ----------
        PyTorchNetwork
            The PyTorchNetwork resulting from the application of the pure pruning procedure.

        &#34;&#34;&#34;

        # We transfer the internal pytorch model to the CPU for the pruning procedure.
        net.pytorch_network.cpu()

        # We compute the total number of weights in the batch normalization layers (which, for fully connected networks,
        # is equal to the number of neurons in the corresponding fully-connected layer).
        num_bn_weights = 0
        for m in net.pytorch_network.modules():
            if isinstance(m, torch.nn.BatchNorm1d):
                num_bn_weights += m.weight.numel()

        # We copy all the absolute values of the batch norm weights in a new tensor and we sort in ascending order
        bn_weights = torch.zeros(num_bn_weights)
        bn_weights_index = 0
        for m in net.pytorch_network.modules():
            if isinstance(m, torch.nn.BatchNorm1d):
                size = m.weight.numel()
                bn_weights[bn_weights_index:(bn_weights_index + size)] = m.weight.abs().clone()
                bn_weights_index += size

        ordered_bn_weights, ordered_bn_indexes = torch.sort(bn_weights)

        # We determine the number of neurons we need to remove, given the sparsity rate.
        threshold_index = math.floor(num_bn_weights * self.sparsity_rate)

        # We select the batch norm weight absolute value we will use as threshold value given the threshold index.
        threshold_value = ordered_bn_weights[threshold_index]

        # We now need to create a new network with the correct number of neurons in the different layers.
        # To do so we assume that in the network after a linear layer there is always a batch norm layer.

        new_layers = []
        previous_layer_mask = None
        old_layers = [m for m in net.pytorch_network.modules()]
        num_layers = len(old_layers)
        for i in range(num_layers):

            if (i == num_layers - 1) and isinstance(old_layers[i], torch.nn.Linear):

                # In this case we are considering the last layer of the network (which we assume to be a linear layer),
                # therefore the number of output of the new layer will be equal to the one of the old layer.

                previous_nonzero_indexes = previous_layer_mask.nonzero(as_tuple=True)[0]

                # If the old linear layer had bias then also the new linear layer has them.
                if old_layers[i].bias is None:
                    has_bias = False
                else:
                    has_bias = True

                # The number of input features for the new linear layer is equal to the number of non-zero elements in
                # the mask of the previous layer.
                num_in_features = int(previous_layer_mask.sum().item())

                # We create the new linear layer with the correct architecture.
                print(num_in_features, old_layers[i].out_features, has_bias)
                new_linear_layer = torch.nn.Linear(num_in_features, old_layers[i].out_features, has_bias)

                # We copy the parameters corresponding to the still existing neurons.
                new_linear_layer.weight.data = old_layers[i].weight[:, previous_nonzero_indexes].clone()

                if has_bias:
                    new_linear_layer.bias.data = old_layers[i].bias.data

                # We save the new linear layer.
                new_layers.append(new_linear_layer)

            elif isinstance(old_layers[i], torch.nn.Linear) and isinstance(old_layers[i + 1], torch.nn.BatchNorm1d):

                # If the layer old_layers[i] is the first linear layer then the previous layer mask corrspond to the
                # complete input.
                if previous_layer_mask is None:
                    previous_layer_mask = torch.ones(old_layers[i].in_features)

                # We compute the mask corresponding to the batch normalization layer.
                layer_mask = old_layers[i + 1].weight.abs().gt(threshold_value).float()
                new_neuron_number = int(layer_mask.sum().item())

                # We compute the indexes of the non-zero weights for the current batch norm layer and the previous one.
                current_nonzero_indexes = layer_mask.nonzero(as_tuple=True)[0]
                previous_nonzero_indexes = previous_layer_mask.nonzero(as_tuple=True)[0]

                # We create the new batch norm layer with the new neuron number.

                new_bn_layer = torch.nn.BatchNorm1d(new_neuron_number, eps=old_layers[i + 1].eps,
                                                    momentum=old_layers[i + 1].momentum,
                                                    affine=old_layers[i + 1].affine,
                                                    track_running_stats=old_layers[i + 1].track_running_stats)

                # We copy the parameters corresponding to the still existing neurons from the old batch norm layer
                # to the new one. They are identified by the indexes in current_nonzero_indexes.

                new_bn_layer.weight.data = old_layers[i + 1].weight[current_nonzero_indexes].clone()
                new_bn_layer.bias.data = old_layers[i + 1].bias[current_nonzero_indexes].clone()
                new_bn_layer.running_mean = old_layers[i + 1].running_mean[current_nonzero_indexes].clone()
                new_bn_layer.running_var = old_layers[i + 1].running_var[current_nonzero_indexes].clone()

                # If the old linear layer had bias then also the new linear layer has them.
                if old_layers[i].bias is None:
                    has_bias = False
                else:
                    has_bias = True

                # The number of input features for the new linear layer is equal to the number of non-zero elements in
                # the mask of the previous layer.
                num_in_features = int(previous_layer_mask.sum().item())

                # We create the new linear layer with the correct architecture.
                new_linear_layer = torch.nn.Linear(num_in_features, new_neuron_number, has_bias)

                # We copy the parameters corresponding to the still existing neurons.
                new_linear_layer.weight.data = old_layers[i].weight[current_nonzero_indexes, :].clone()
                new_linear_layer.weight.data = new_linear_layer.weight[:, previous_nonzero_indexes].clone()

                if has_bias:
                    new_linear_layer.bias.data = old_layers[i].bias[current_nonzero_indexes]

                # We save the new layers in the order in which they should be in our sequential model: first the linear
                # layer and then the batch norm layer.
                new_layers.append(new_linear_layer)
                new_layers.append(new_bn_layer)

                # We update the value of previous_layer_mask with the current mask.
                previous_layer_mask = layer_mask

            elif isinstance(old_layers[i], torch.nn.Linear) and not isinstance(old_layers[i + 1], torch.nn.BatchNorm1d):

                # If the layer old_layers[i] is the first linear layer then the previous layer mask correspond to the
                # complete input.
                if previous_layer_mask is None:
                    previous_layer_mask = torch.ones(old_layers[i].in_features)

                # If the linear layer is not followed by a batch normalization layer then it will not be neuron pruned,
                # therefore the layer_mask will be equals to the number of output features of the old layer.
                layer_mask = torch.ones(old_layers[i].out_features)

                # We compute the indexes of the non-zero weights for the current batch norm layer and the previous one.
                current_nonzero_indexes = layer_mask.nonzero(as_tuple=True)[0]
                previous_nonzero_indexes = previous_layer_mask.nonzero(as_tuple=True)[0]

                if old_layers[i].bias is None:
                    has_bias = False
                else:
                    has_bias = True

                # The number of input features for the new linear layer is equal to the number of non-zero elements in
                # the mask of the previous layer.
                num_in_features = previous_layer_mask.sum().item()

                # We create the new linear layer with the correct architecture.
                new_linear_layer = torch.nn.Linear(num_in_features, old_layers[i].out_features, has_bias)

                # We copy the parameters corresponding to the still existing neurons.
                new_linear_layer.weight.data = old_layers[i].weight[current_nonzero_indexes, :].clone()
                new_linear_layer.weight.data = new_linear_layer.weight[:, previous_nonzero_indexes].clone()

                if has_bias:
                    new_linear_layer.bias.data = old_layers[i].bias[current_nonzero_indexes]

                # We save the new layers in the order in which they should be in our sequential model: first the linear
                # layer and then the batch norm layer.
                new_layers.append(new_linear_layer)

            elif isinstance(old_layers[i], nn.ReLU):
                new_layers.append(nn.ReLU())

        pruned_network = torch.nn.Sequential(*new_layers)
        net.pytorch_network = pruned_network
        net.identifier = net.identifier + &#39;_pruned&#39;
        return net</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="pynever.strategies.pruning.PruningStrategy" href="#pynever.strategies.pruning.PruningStrategy">PruningStrategy</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="pynever.strategies.pruning.NetworkSlimming.prune"><code class="name flex">
<span>def <span class="ident">prune</span></span>(<span>self, network: <a title="pynever.networks.NeuralNetwork" href="../networks.html#pynever.networks.NeuralNetwork">NeuralNetwork</a>, dataset: <a title="pynever.datasets.Dataset" href="../datasets.html#pynever.datasets.Dataset">Dataset</a>) ‑> <a title="pynever.networks.NeuralNetwork" href="../networks.html#pynever.networks.NeuralNetwork">NeuralNetwork</a></span>
</code></dt>
<dd>
<div class="desc"><p>Prune the neural network of interest using the pruning strategy Network Slimming.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>network</code></strong> :&ensp;<code>NeuralNetwork</code></dt>
<dd>The neural network to prune.</dd>
<dt><strong><code>dataset</code></strong> :&ensp;<code>Dataset</code></dt>
<dd>The dataset to use for the pre-training and fine-tuning procedure.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>NeuralNetwork</code></dt>
<dd>The Neural Network resulting from the application of the pruning strategy to the original network.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prune(self, network: networks.NeuralNetwork, dataset: datasets.Dataset) -&gt; networks.NeuralNetwork:
    &#34;&#34;&#34;
    Prune the neural network of interest using the pruning strategy Network Slimming.

    Parameters
    ----------
    network : NeuralNetwork
        The neural network to prune.
    dataset: Dataset
        The dataset to use for the pre-training and fine-tuning procedure.

    Returns
    ----------
    NeuralNetwork
        The Neural Network resulting from the application of the pruning strategy to the original network.

    &#34;&#34;&#34;

    if self.training_strategy is not None and self.pre_training:
        fine_tuning = self.training_strategy.fine_tuning
        self.training_strategy.fine_tuning = False
        network = self.training_strategy.train(network, dataset)
        self.training_strategy.fine_tuning = fine_tuning

    pytorch_converter = cv.PyTorchConverter()
    py_net = pytorch_converter.from_neural_network(network)

    py_net = self.__pruning(py_net)

    network.alt_rep_cache.clear()
    network.alt_rep_cache.append(py_net)
    network.up_to_date = False

    if self.training_strategy is not None and self.training_strategy.fine_tuning:
        old_l1_decay = self.training_strategy.l1_decay
        old_batchnorm_decay = self.training_strategy.batchnorm_decay
        self.training_strategy.l1_decay = 0
        self.training_strategy.batchnorm_decay = 0
        network = self.training_strategy.train(network, dataset)
        self.training_strategy.l1_decay = old_l1_decay
        self.training_strategy.batchnorm_decay = old_batchnorm_decay

    return network</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pynever.strategies.pruning.PruningStrategy"><code class="flex name class">
<span>class <span class="ident">PruningStrategy</span></span>
</code></dt>
<dd>
<div class="desc"><p>An abstract class used to represent a Pruning Strategy.</p>
<h2 id="methods">Methods</h2>
<p>prune(NeuralNetwork, Dataset)
Prune the neural network of interest using a pruning strategy determined in the concrete children.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PruningStrategy(abc.ABC):
    &#34;&#34;&#34;
    An abstract class used to represent a Pruning Strategy.

    Methods
    ----------
    prune(NeuralNetwork, Dataset)
        Prune the neural network of interest using a pruning strategy determined in the concrete children.

    &#34;&#34;&#34;

    @abc.abstractmethod
    def prune(self, network: networks.NeuralNetwork, dataset: datasets.Dataset) -&gt; networks.NeuralNetwork:
        &#34;&#34;&#34;
        Prune the neural network of interest using a pruning strategy determined in the concrete children.

        Parameters
        ----------
        network : NeuralNetwork
            The neural network to prune.
        dataset: Dataset
            The dataset to use for the pre-training and fine-tuning procedure.

        Returns
        ----------
        NeuralNetwork
            The Neural Network resulting from the application of the pruning strategy to the original network.

        &#34;&#34;&#34;
        pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="pynever.strategies.pruning.NetworkSlimming" href="#pynever.strategies.pruning.NetworkSlimming">NetworkSlimming</a></li>
<li><a title="pynever.strategies.pruning.WeightPruning" href="#pynever.strategies.pruning.WeightPruning">WeightPruning</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="pynever.strategies.pruning.PruningStrategy.prune"><code class="name flex">
<span>def <span class="ident">prune</span></span>(<span>self, network: <a title="pynever.networks.NeuralNetwork" href="../networks.html#pynever.networks.NeuralNetwork">NeuralNetwork</a>, dataset: <a title="pynever.datasets.Dataset" href="../datasets.html#pynever.datasets.Dataset">Dataset</a>) ‑> <a title="pynever.networks.NeuralNetwork" href="../networks.html#pynever.networks.NeuralNetwork">NeuralNetwork</a></span>
</code></dt>
<dd>
<div class="desc"><p>Prune the neural network of interest using a pruning strategy determined in the concrete children.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>network</code></strong> :&ensp;<code>NeuralNetwork</code></dt>
<dd>The neural network to prune.</dd>
<dt><strong><code>dataset</code></strong> :&ensp;<code>Dataset</code></dt>
<dd>The dataset to use for the pre-training and fine-tuning procedure.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>NeuralNetwork</code></dt>
<dd>The Neural Network resulting from the application of the pruning strategy to the original network.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abc.abstractmethod
def prune(self, network: networks.NeuralNetwork, dataset: datasets.Dataset) -&gt; networks.NeuralNetwork:
    &#34;&#34;&#34;
    Prune the neural network of interest using a pruning strategy determined in the concrete children.

    Parameters
    ----------
    network : NeuralNetwork
        The neural network to prune.
    dataset: Dataset
        The dataset to use for the pre-training and fine-tuning procedure.

    Returns
    ----------
    NeuralNetwork
        The Neural Network resulting from the application of the pruning strategy to the original network.

    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pynever.strategies.pruning.WeightPruning"><code class="flex name class">
<span>class <span class="ident">WeightPruning</span></span>
<span>(</span><span>sparsity_rate: float, training_strategy: <a title="pynever.strategies.training.AdamTraining" href="training.html#pynever.strategies.training.AdamTraining">AdamTraining</a> = None, pre_training: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>A concrete class used to represent the weight pruning strategy.
This kind of pruning select the least important weights of the neural network
of interest and set them to 0.
We refer to <a href="https://arxiv.org/abs/1506.02626">https://arxiv.org/abs/1506.02626</a> for theoretical details on the strategy.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>sparsity_rate</code></strong> :&ensp;<code>float</code></dt>
<dd>It determines the percentage of neurons which will be removed. It must be a Real number between 0 and 1.</dd>
<dt><strong><code>training_strategy</code></strong> :&ensp;<code>AdamTraining</code></dt>
<dd>The training strategy to use for pre-training and/or fine-tuning.</dd>
<dt><strong><code>pre_training</code></strong> :&ensp;<code>bool</code></dt>
<dd>Flag to indicate if the network need to be pre-trained.</dd>
</dl>
<h2 id="methods">Methods</h2>
<p>prune(NeuralNetwork, Dataset)
Prune the neural network of interest using the pruning strategy Weight Pruning.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class WeightPruning(PruningStrategy):
    &#34;&#34;&#34;
    A concrete class used to represent the weight pruning strategy.
    This kind of pruning select the least important weights of the neural network
    of interest and set them to 0.
    We refer to https://arxiv.org/abs/1506.02626 for theoretical details on the strategy.

    Attributes
    ----------
    sparsity_rate : float
        It determines the percentage of neurons which will be removed. It must be a Real number between 0 and 1.
    training_strategy : AdamTraining
        The training strategy to use for pre-training and/or fine-tuning.
    pre_training : bool
        Flag to indicate if the network need to be pre-trained.

    Methods
    ----------
    prune(NeuralNetwork, Dataset)
        Prune the neural network of interest using the pruning strategy Weight Pruning.

    &#34;&#34;&#34;

    def __init__(self, sparsity_rate: float, training_strategy: training.AdamTraining = None,
                 pre_training: bool = False):

        self.sparsity_rate = sparsity_rate
        self.training_strategy = training_strategy
        self.pre_training = pre_training

    def prune(self, network: networks.NeuralNetwork, dataset: datasets.Dataset) -&gt; networks.NeuralNetwork:
        &#34;&#34;&#34;
        Prune the neural network of interest using the pruning strategy Weight Pruning.

        Parameters
        ----------
        network : NeuralNetwork
            The neural network to prune.
        dataset : Dataset
            The dataset to use for the pre-training and fine-tuning procedure.

        Returns
        ----------
        NeuralNetwork
            The Neural Network resulting from the application of the pruning strategy to the original network.

        &#34;&#34;&#34;

        if self.training_strategy is not None and self.pre_training:
            fine_tuning = self.training_strategy.fine_tuning
            self.training_strategy.fine_tuning = False
            network = self.training_strategy.train(network, dataset)
            self.training_strategy.fine_tuning = fine_tuning

        pytorch_converter = cv.PyTorchConverter()
        py_net = pytorch_converter.from_neural_network(network)

        py_net = self.__pruning(py_net)

        network.alt_rep_cache.clear()
        network.alt_rep_cache.append(py_net)
        network.up_to_date = False

        if self.training_strategy is not None and self.training_strategy.fine_tuning:
            old_l1_decay = self.training_strategy.l1_decay
            old_batchnorm_decay = self.training_strategy.batchnorm_decay
            self.training_strategy.l1_decay = 0
            self.training_strategy.batchnorm_decay = 0
            network = self.training_strategy.train(network, dataset)
            self.training_strategy.l1_decay = old_l1_decay
            self.training_strategy.batchnorm_decay = old_batchnorm_decay

        return network

    def __pruning(self, net: cv.PyTorchNetwork):
        &#34;&#34;&#34;
        Procedure for the pruning of the weights of the PyTorchNetwork passed as an argument.

        Parameters
        ----------
        net : PyTorchNetwork
            The PyTorchNetwork to prune.

        Returns
        ----------
        PyTorchNetwork
            The pruned PyTorchNetwork.

        &#34;&#34;&#34;

        # We transfer the internal pytorch model to the CPU for the pruning procedure.
        net.pytorch_network.cpu()

        # We compute the number of weights in the network
        num_weights = 0
        for m in net.pytorch_network.modules():

            if isinstance(m, torch.nn.Linear):
                num_weights += m.weight.numel()

        # We copy all the absolute values of the weights in a new tensor and we sort in ascending order
        weights = torch.zeros(num_weights)
        index = 0
        for m in net.pytorch_network.modules():

            if isinstance(m, torch.nn.Linear):
                size = m.weight.numel()
                weights[index:(index + size)] = m.weight.view(-1).abs().clone()
                index += size

        ordered_weights, ordered_indexes = torch.sort(weights)

        # We determine the number of weights we need to set to 0, given the sparsity rate.
        threshold_index = math.floor(num_weights * self.sparsity_rate)

        # We select the weight absolute value we will use as threshold value given the threshold index.
        threshold_value = ordered_weights[threshold_index]

        # We set all the weights of the different layers to 0 if they are less or equal than the threshold value
        # (in absolute value)

        for m in net.pytorch_network.modules():

            if isinstance(m, torch.nn.Linear):
                # The values of the mask are 0 when the corresponding weight is less then the threshold_value (in
                # absolute value), otherwise they are 1
                mask = m.weight.abs().gt(threshold_value).float()
                m.weight.data = torch.mul(m.weight, mask)

        return net</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="pynever.strategies.pruning.PruningStrategy" href="#pynever.strategies.pruning.PruningStrategy">PruningStrategy</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="pynever.strategies.pruning.WeightPruning.prune"><code class="name flex">
<span>def <span class="ident">prune</span></span>(<span>self, network: <a title="pynever.networks.NeuralNetwork" href="../networks.html#pynever.networks.NeuralNetwork">NeuralNetwork</a>, dataset: <a title="pynever.datasets.Dataset" href="../datasets.html#pynever.datasets.Dataset">Dataset</a>) ‑> <a title="pynever.networks.NeuralNetwork" href="../networks.html#pynever.networks.NeuralNetwork">NeuralNetwork</a></span>
</code></dt>
<dd>
<div class="desc"><p>Prune the neural network of interest using the pruning strategy Weight Pruning.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>network</code></strong> :&ensp;<code>NeuralNetwork</code></dt>
<dd>The neural network to prune.</dd>
<dt><strong><code>dataset</code></strong> :&ensp;<code>Dataset</code></dt>
<dd>The dataset to use for the pre-training and fine-tuning procedure.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>NeuralNetwork</code></dt>
<dd>The Neural Network resulting from the application of the pruning strategy to the original network.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prune(self, network: networks.NeuralNetwork, dataset: datasets.Dataset) -&gt; networks.NeuralNetwork:
    &#34;&#34;&#34;
    Prune the neural network of interest using the pruning strategy Weight Pruning.

    Parameters
    ----------
    network : NeuralNetwork
        The neural network to prune.
    dataset : Dataset
        The dataset to use for the pre-training and fine-tuning procedure.

    Returns
    ----------
    NeuralNetwork
        The Neural Network resulting from the application of the pruning strategy to the original network.

    &#34;&#34;&#34;

    if self.training_strategy is not None and self.pre_training:
        fine_tuning = self.training_strategy.fine_tuning
        self.training_strategy.fine_tuning = False
        network = self.training_strategy.train(network, dataset)
        self.training_strategy.fine_tuning = fine_tuning

    pytorch_converter = cv.PyTorchConverter()
    py_net = pytorch_converter.from_neural_network(network)

    py_net = self.__pruning(py_net)

    network.alt_rep_cache.clear()
    network.alt_rep_cache.append(py_net)
    network.up_to_date = False

    if self.training_strategy is not None and self.training_strategy.fine_tuning:
        old_l1_decay = self.training_strategy.l1_decay
        old_batchnorm_decay = self.training_strategy.batchnorm_decay
        self.training_strategy.l1_decay = 0
        self.training_strategy.batchnorm_decay = 0
        network = self.training_strategy.train(network, dataset)
        self.training_strategy.l1_decay = old_l1_decay
        self.training_strategy.batchnorm_decay = old_batchnorm_decay

    return network</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pynever.strategies" href="index.html">pynever.strategies</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="pynever.strategies.pruning.NetworkSlimming" href="#pynever.strategies.pruning.NetworkSlimming">NetworkSlimming</a></code></h4>
<ul class="">
<li><code><a title="pynever.strategies.pruning.NetworkSlimming.prune" href="#pynever.strategies.pruning.NetworkSlimming.prune">prune</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pynever.strategies.pruning.PruningStrategy" href="#pynever.strategies.pruning.PruningStrategy">PruningStrategy</a></code></h4>
<ul class="">
<li><code><a title="pynever.strategies.pruning.PruningStrategy.prune" href="#pynever.strategies.pruning.PruningStrategy.prune">prune</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pynever.strategies.pruning.WeightPruning" href="#pynever.strategies.pruning.WeightPruning">WeightPruning</a></code></h4>
<ul class="">
<li><code><a title="pynever.strategies.pruning.WeightPruning.prune" href="#pynever.strategies.pruning.WeightPruning.prune">prune</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>