<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>pynever.utilities API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pynever.utilities</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import pynever.strategies.conversion as cv
import pynever.nodes as nodes
import pynever.networks as networks
import pynever.datasets as datasets
import torch
import math
import torch.nn.functional as funct
from pynever.tensor import Tensor
import numpy as np


def combine_batchnorm1d(linear: nodes.FullyConnectedNode, batchnorm: nodes.BatchNorm1DNode) -&gt; nodes.FullyConnectedNode:
    &#34;&#34;&#34;
    Utility function to combine a BatchNorm1DNode node with a FullyConnectedNode in a corresponding FullyConnectedNode.

    Parameters
    ----------
    linear : FullyConnectedNode
        FullyConnectedNode to combine.
    batchnorm : BatchNorm1DNode
        BatchNorm1DNode to combine.

    Return
    ----------
    FullyConnectedNode
        The FullyConnectedNode resulting from the fusion of the two input nodes.

    &#34;&#34;&#34;

    l_weight = torch.from_numpy(linear.weight)
    l_bias = torch.from_numpy(linear.bias)
    bn_running_mean = torch.from_numpy(batchnorm.running_mean)
    bn_running_var = torch.from_numpy(batchnorm.running_var)
    bn_weight = torch.from_numpy(batchnorm.weight)
    bn_bias = torch.from_numpy(batchnorm.bias)
    bn_eps = batchnorm.eps

    fused_bias = torch.div(bn_weight, torch.sqrt(bn_running_var + bn_eps))
    fused_bias = torch.mul(fused_bias, torch.sub(l_bias, bn_running_mean))
    fused_bias = torch.add(fused_bias, bn_bias)

    fused_weight = torch.diag(torch.div(bn_weight, torch.sqrt(bn_running_var + bn_eps)))
    fused_weight = torch.matmul(fused_weight, l_weight)

    fused_linear = nodes.FullyConnectedNode(linear.identifier, linear.in_features, linear.out_features, fused_weight.numpy(),
                                            fused_bias.numpy())

    return fused_linear


def combine_batchnorm1d_net(network: networks.SequentialNetwork) -&gt; networks.SequentialNetwork:
    &#34;&#34;&#34;
    Utilities function to combine all the FullyConnectedNodes followed by BatchNorm1DNodes in corresponding
    FullyConnectedNodes.

    Parameters
    ----------
    network : SequentialNetwork
        Sequential Network of interest of which we want to combine the nodes.

    Return
    ----------
    SequentialNetwork
        Corresponding Sequential Network with the combined nodes.

    &#34;&#34;&#34;

    if not network.up_to_date:

        for alt_rep in network.alt_rep_cache:

            if alt_rep.up_to_date:

                if isinstance(alt_rep, cv.PyTorchNetwork):
                    pytorch_cv = cv.PyTorchConverter()
                    network = pytorch_cv.to_neural_network(alt_rep)
                elif isinstance(alt_rep, cv.ONNXNetwork):
                    onnx_cv = cv.ONNXConverter
                    network = onnx_cv.to_neural_network(alt_rep)
                else:
                    raise NotImplementedError
                break

    combined_network = networks.SequentialNetwork(network.identifier + &#39;_combined&#39;)

    current_node = network.get_first_node()
    node_index = 1
    while network.get_next_node(current_node) is not None and current_node is not None:

        next_node = network.get_next_node(current_node)
        if isinstance(current_node, nodes.FullyConnectedNode) and isinstance(next_node, nodes.BatchNorm1DNode):
            combined_node = combine_batchnorm1d(current_node, next_node)
            combined_node.identifier = f&#34;Combined_Linear_{node_index}&#34;
            combined_network.add_node(combined_node)
            next_node = network.get_next_node(next_node)

        elif isinstance(current_node, nodes.FullyConnectedNode):
            identifier = f&#34;Linear_{node_index}&#34;
            new_node = nodes.FullyConnectedNode(identifier, current_node.in_features, current_node.out_features,
                                                current_node.weight, current_node.bias)
            combined_network.add_node(new_node)

        elif isinstance(current_node, nodes.ReLUNode):
            identifier = f&#34;ReLU_{node_index}&#34;
            new_node = nodes.ReLUNode(identifier, current_node.num_features)
            combined_network.add_node(new_node)
        else:
            raise NotImplementedError

        node_index += 1
        current_node = next_node

    if isinstance(current_node, nodes.FullyConnectedNode):
        identifier = f&#34;Linear_{node_index}&#34;
        new_node = nodes.FullyConnectedNode(identifier, current_node.in_features, current_node.out_features,
                                            current_node.weight, current_node.bias)
        combined_network.add_node(new_node)
    elif isinstance(current_node, nodes.ReLUNode):
        identifier = f&#34;ReLU_{node_index}&#34;
        new_node = nodes.ReLUNode(identifier, current_node.num_features)
        combined_network.add_node(new_node)
    else:
        raise NotImplementedError

    return combined_network


def testing(net: cv.PyTorchNetwork, dataset: datasets.Dataset, test_batch_size: int, cuda: bool) -&gt; (float, float):
    &#34;&#34;&#34;
    Testing procedure for a PyTorchNetwork.

    Parameters
    ----------
    net : PyTorchNetwork
        Neural Network to test.
    dataset : Dataset
        Dataset used for testing the network.
    test_batch_size : int
        Dimension for the test batch size for the testing procedure
    cuda : bool
        Whether to use the cuda library for the procedure (default: False).

    Returns
    ----------
    (float, float)
        Rate of correct samples and loss.

    &#34;&#34;&#34;

    if cuda:
        net.pytorch_network.cuda()
    else:
        net.pytorch_network.cpu()

    test_set = dataset.get_test_set()

    net.pytorch_network.eval()
    net.pytorch_network.float()
    test_loss = 0
    correct = 0
    with torch.no_grad():

        batch_idx = 0
        data_idx = 0

        while data_idx &lt; len(test_set[0]):

            if data_idx + test_batch_size &gt;= len(test_set[0]):
                last_data_idx = len(test_set[0])
            else:
                last_data_idx = data_idx + test_batch_size

            data = torch.from_numpy(test_set[0][data_idx:last_data_idx, :])
            target = torch.from_numpy(test_set[1][data_idx:last_data_idx])

            if cuda:
                data, target = data.cuda(), target.cuda()

            data, target = torch.autograd.Variable(data), torch.autograd.Variable(target)
            output = net.pytorch_network(data)
            test_loss += funct.cross_entropy(output, target, reduction=&#39;sum&#39;).data.item()  # sum up batch loss
            pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability
            correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()
            batch_idx += 1
            data_idx += test_batch_size

    test_loss /= float(math.floor(len(test_set[0]) / test_batch_size))
    print(&#39;\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.1f}%)\n&#39;.format(
        test_loss, correct, len(test_set[0]),
        100. * correct / len(test_set[0])))

    return correct / float(len(test_set[0])), test_loss


def generate_targeted_linf_robustness_query(data: Tensor, adv_target: int, bounds: tuple,
                                            num_classes: int, epsilon: float, filepath: str):
    &#34;&#34;&#34;
    Function to generate a targeted Robustness SMTLIB query and to save it to a SMTLIB file.
    The robustness query is of the kind based on the infinity norm.
    It assumes that the data and target are from a classification task.

    Parameters
    ----------
    data : Tensor
        Input data of interest.
    adv_target : int
        Desired adversarial target for the input data.
    bounds : (int, int)
        Bounds for the input data (lower_bound, upper_bound).
    num_classes : int
        Number of possible classes.
    epsilon : float
        Perturbation with respect to the infinity norm.
    filepath : str
        Filepath for the resulting SMTLIB file.

    &#34;&#34;&#34;
    with open(filepath, &#34;w&#34;) as f:

        flattened_data = data.flatten()
        for i in range(len(flattened_data)):
            f.write(f&#34;(declare-const X_{i} Real)\n&#34;)

        for i in range(num_classes):
            f.write(f&#34;(declare-const Y_{i} Real)\n&#34;)

        for i in range(len(flattened_data)):

            if flattened_data[i] - epsilon &lt; bounds[0]:
                f.write(f&#34;(assert (&gt;= X_{i} {bounds[0]}))\n&#34;)
            else:
                f.write(f&#34;(assert (&gt;= X_{i} {flattened_data[i] - epsilon}))\n&#34;)

            if flattened_data[i] + epsilon &gt; bounds[1]:
                f.write(f&#34;(assert (&lt;= X_{i} {bounds[1]}))\n&#34;)
            else:
                f.write(f&#34;(assert (&lt;= X_{i} {flattened_data[i] + epsilon}))\n&#34;)

        for i in range(num_classes):

            if i != adv_target:
                f.write(f&#34;(assert (&lt;= (- Y_{i} Y_{adv_target}) 0))\n&#34;)

def generate_untargeted_linf_robustness_query(data: Tensor, target: int, bounds: tuple,
                                              num_classes: int, epsilon: float, filepath: str):
    &#34;&#34;&#34;
    Function to generate an untargeted Robustness SMTLIB query and to save it to a SMTLIB file.
    The robustness query is of the kind based on the infinity norm.
    It assumes that the data and target are from a classification task.

    Parameters
    ----------
    data : Tensor
        Input data of interest.
    adv_target : int
        Desired adversarial target for the input data.
    bounds : (int, int)
        Bounds for the input data (lower_bound, upper_bound).
    num_classes : int
        Number of possible classes.
    epsilon : float
        Perturbation with respect to the infinity norm.
    filepath : str
        Filepath for the resulting SMTLIB file.

    &#34;&#34;&#34;
    with open(filepath, &#34;w&#34;) as f:

        flattened_data = data.flatten()
        for i in range(len(flattened_data)):
            f.write(f&#34;(declare-const X_{i} Real)\n&#34;)

        for i in range(num_classes):
            f.write(f&#34;(declare-const Y_{i} Real)\n&#34;)

        for i in range(len(flattened_data)):

            if flattened_data[i] - epsilon &lt; bounds[0]:
                f.write(f&#34;(assert (&gt;= X_{i} {bounds[0]}))\n&#34;)
            else:
                f.write(f&#34;(assert (&gt;= X_{i} {flattened_data[i] - epsilon}))\n&#34;)

            if flattened_data[i] + epsilon &gt; bounds[1]:
                f.write(f&#34;(assert (&lt;= X_{i} {bounds[1]}))\n&#34;)
            else:
                f.write(f&#34;(assert (&lt;= X_{i} {flattened_data[i] + epsilon}))\n&#34;)

        output_query = &#34;(assert (or&#34;
        for i in range(num_classes):

            if i != target:
                output_query += f&#34; (&lt;= (- Y_{target} Y_{i}) 0)&#34;

        output_query += &#34;))&#34;
        f.write(output_query)


def parse_linf_robustness_smtlib(filepath: str) -&gt; (bool, list, int):
    &#34;&#34;&#34;
    Function to extract the parameters of a robustness query from the smtlib file.
    It assume the SMTLIB file is structured as following:

        ; definition of the variables of interest
        (declare-const X_0 Real)
        (declare-const X_1 Real)
        ...
        (declare-const Y_1 Real)
        (declare-const Y_2 Real)
        ...
        ; definition of the constraints
        (assert (&gt;= X_0 eps_0))
        (assert (&lt;= X_0 eps_1))
        ...
        (assert (&lt;= (- Y_0 Y_1) 0))
        ...

    Where the eps_i are Real numbers.

    Parameters
    ----------
    filepath : str
        Filepath to the SMTLIB file.

    Returns
    ----------
    (bool, list, int)
        Tuple of list: the first list contains the values eps_i for each variables as tuples (lower_bound, upper_bound),
        while the int correspond to the desired target for the related data.
    &#34;&#34;&#34;
    targeted = True
    correct_target = -1
    lb = []
    ub = []
    with open(filepath, &#39;r&#39;) as f:

        for line in f:

            line = line.replace(&#39;(&#39;, &#39;( &#39;)
            line = line.replace(&#39;)&#39;, &#39; )&#39;)
            if line[0] == &#39;(&#39;:
                aux = line.split()
                if aux[1] == &#39;assert&#39;:

                    if aux[4] == &#39;(&#39;:
                        if aux[3] == &#39;or&#39;:
                            targeted = False
                            temp = aux[8].split(&#34;_&#34;)
                            correct_target = int(temp[1])
                        else:
                            targeted = True
                            temp = aux[7].split(&#34;_&#34;)
                            correct_target = int(temp[1])

                    else:

                        if aux[3] == &#34;&gt;=&#34;:
                            lb.append(float(aux[5]))
                        else:
                            ub.append(float(aux[5]))

    input_bounds = []
    for i in range(len(lb)):
        input_bounds.append((lb[i], ub[i]))

    return targeted, input_bounds, correct_target


def net_update(network: networks.NeuralNetwork) -&gt; networks.NeuralNetwork:

    if not network.up_to_date:

        for alt_rep in network.alt_rep_cache:

            if alt_rep.up_to_date:
                if isinstance(alt_rep, cv.ONNXNetwork):
                    return cv.ONNXConverter().to_neural_network(alt_rep)
                elif isinstance(alt_rep, cv.PyTorchNetwork):
                    return cv.PyTorchConverter().to_neural_network(alt_rep)
                else:
                    raise NotImplementedError

    else:
        return network


def parse_acas_property(filepath: str) -&gt; ((Tensor, Tensor), (Tensor, Tensor)):

    in_coeff = np.zeros((10, 5))
    in_bias = np.zeros((10, 1))
    out_coeff = []
    out_bias = []
    row_index = 0

    with open(filepath, &#39;r&#39;) as f:

        for line in f:

            if line[0] == &#34;x&#34;:
                splitted_line = line.split(&#34; &#34;)
                var_index = int(splitted_line[0][1])
                if splitted_line[1] == &#34;&gt;=&#34;:
                    in_coeff[row_index, var_index] = -1
                    in_bias[row_index] = -float(splitted_line[2])
                else:
                    in_coeff[row_index, var_index] = 1
                    in_bias[row_index] = float(splitted_line[2])

            else:

                splitted_line = line.split(&#34; &#34;)
                if len(splitted_line) == 3:
                    var_index = int(splitted_line[0][1])
                    temp = np.zeros(5)
                    if splitted_line[1] == &#34;&gt;=&#34;:
                        temp[var_index] = -1
                        out_coeff.append(temp)
                        out_bias.append(-float(splitted_line[2]))
                    else:
                        temp[var_index] = 1
                        out_coeff.append(temp)
                        out_bias.append(float(splitted_line[2]))
                else:
                    var_index_1 = int(splitted_line[0][2])
                    var_index_2 = int(splitted_line[1][2])
                    temp = np.zeros(5)
                    if splitted_line[2] == &#34;&gt;=&#34;:
                        temp[var_index_1] = -1
                        temp[var_index_2] = 1
                        out_coeff.append(temp)
                        out_bias.append(-float(splitted_line[3]))
                    else:
                        temp[var_index_1] = 1
                        temp[var_index_2] = -1
                        out_coeff.append(temp)
                        out_bias.append(float(splitted_line[3]))

            row_index = row_index + 1

        out_coeff = np.array(out_coeff)
        array_out_bias = np.zeros((len(out_bias), 1))

        for i in range(len(out_bias)):
            array_out_bias[i, 0] = out_bias[i]

        out_bias = array_out_bias

    return (in_coeff, in_bias), (out_coeff, out_bias)


def parse_nnet(filepath: str) -&gt; (list, list, list, list, list, list):

    with open(filepath) as f:

        line = f.readline()
        cnt = 1
        while line[0:2] == &#34;//&#34;:
            line = f.readline()
            cnt += 1
        # numLayers does&#39;t include the input layer!
        numLayers, inputSize, outputSize, maxLayersize = [int(x) for x in line.strip().split(&#34;,&#34;)[:-1]]
        line = f.readline()

        # input layer size, layer1size, layer2size...
        layerSizes = [int(x) for x in line.strip().split(&#34;,&#34;)[:-1]]

        line = f.readline()
        symmetric = int(line.strip().split(&#34;,&#34;)[0])

        line = f.readline()
        inputMinimums = [float(x) for x in line.strip().split(&#34;,&#34;)[:-1]]

        line = f.readline()
        inputMaximums = [float(x) for x in line.strip().split(&#34;,&#34;)[:-1]]

        line = f.readline()
        means = [float(x) for x in line.strip().split(&#34;,&#34;)[:-1]]

        line = f.readline()
        ranges = [float(x) for x in line.strip().split(&#34;,&#34;)[:-1]]

        weights = []
        biases = []
        for layernum in range(numLayers):

            previousLayerSize = layerSizes[layernum]
            currentLayerSize = layerSizes[layernum + 1]
            # weights
            weights.append([])
            biases.append([])
            # weights
            for i in range(currentLayerSize):
                line = f.readline()
                aux = [float(x) for x in line.strip().split(&#34;,&#34;)[:-1]]
                weights[layernum].append([])
                for j in range(previousLayerSize):
                    weights[layernum][i].append(aux[j])
            # biases
            for i in range(currentLayerSize):
                line = f.readline()
                x = float(line.strip().split(&#34;,&#34;)[0])
                biases[layernum].append(x)

        numLayers = numLayers
        layerSizes = layerSizes
        inputSize = inputSize
        outputSize = outputSize
        maxLayersize = maxLayersize
        inputMinimums = inputMinimums
        inputMaximums = inputMaximums
        inputMeans = means[:-1]
        inputRanges = ranges[:-1]
        outputMean = means[-1]
        outputRange = ranges[-1]
        weights = weights
        biases = biases

        new_weights = []
        new_biases = []
        for i in range(numLayers):
            weight = np.array(weights[i])
            bias = np.array(biases[i])

            new_weights.append(weight)
            new_biases.append(bias)

        return new_weights, new_biases, inputMeans, inputRanges, outputMean, outputRange</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="pynever.utilities.combine_batchnorm1d"><code class="name flex">
<span>def <span class="ident">combine_batchnorm1d</span></span>(<span>linear: <a title="pynever.nodes.FullyConnectedNode" href="nodes.html#pynever.nodes.FullyConnectedNode">FullyConnectedNode</a>, batchnorm: <a title="pynever.nodes.BatchNorm1DNode" href="nodes.html#pynever.nodes.BatchNorm1DNode">BatchNorm1DNode</a>) ‑> <a title="pynever.nodes.FullyConnectedNode" href="nodes.html#pynever.nodes.FullyConnectedNode">FullyConnectedNode</a></span>
</code></dt>
<dd>
<div class="desc"><p>Utility function to combine a BatchNorm1DNode node with a FullyConnectedNode in a corresponding FullyConnectedNode.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>linear</code></strong> :&ensp;<code>FullyConnectedNode</code></dt>
<dd>FullyConnectedNode to combine.</dd>
<dt><strong><code>batchnorm</code></strong> :&ensp;<code>BatchNorm1DNode</code></dt>
<dd>BatchNorm1DNode to combine.</dd>
</dl>
<h2 id="return">Return</h2>
<p>FullyConnectedNode
The FullyConnectedNode resulting from the fusion of the two input nodes.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def combine_batchnorm1d(linear: nodes.FullyConnectedNode, batchnorm: nodes.BatchNorm1DNode) -&gt; nodes.FullyConnectedNode:
    &#34;&#34;&#34;
    Utility function to combine a BatchNorm1DNode node with a FullyConnectedNode in a corresponding FullyConnectedNode.

    Parameters
    ----------
    linear : FullyConnectedNode
        FullyConnectedNode to combine.
    batchnorm : BatchNorm1DNode
        BatchNorm1DNode to combine.

    Return
    ----------
    FullyConnectedNode
        The FullyConnectedNode resulting from the fusion of the two input nodes.

    &#34;&#34;&#34;

    l_weight = torch.from_numpy(linear.weight)
    l_bias = torch.from_numpy(linear.bias)
    bn_running_mean = torch.from_numpy(batchnorm.running_mean)
    bn_running_var = torch.from_numpy(batchnorm.running_var)
    bn_weight = torch.from_numpy(batchnorm.weight)
    bn_bias = torch.from_numpy(batchnorm.bias)
    bn_eps = batchnorm.eps

    fused_bias = torch.div(bn_weight, torch.sqrt(bn_running_var + bn_eps))
    fused_bias = torch.mul(fused_bias, torch.sub(l_bias, bn_running_mean))
    fused_bias = torch.add(fused_bias, bn_bias)

    fused_weight = torch.diag(torch.div(bn_weight, torch.sqrt(bn_running_var + bn_eps)))
    fused_weight = torch.matmul(fused_weight, l_weight)

    fused_linear = nodes.FullyConnectedNode(linear.identifier, linear.in_features, linear.out_features, fused_weight.numpy(),
                                            fused_bias.numpy())

    return fused_linear</code></pre>
</details>
</dd>
<dt id="pynever.utilities.combine_batchnorm1d_net"><code class="name flex">
<span>def <span class="ident">combine_batchnorm1d_net</span></span>(<span>network: <a title="pynever.networks.SequentialNetwork" href="networks.html#pynever.networks.SequentialNetwork">SequentialNetwork</a>) ‑> <a title="pynever.networks.SequentialNetwork" href="networks.html#pynever.networks.SequentialNetwork">SequentialNetwork</a></span>
</code></dt>
<dd>
<div class="desc"><p>Utilities function to combine all the FullyConnectedNodes followed by BatchNorm1DNodes in corresponding
FullyConnectedNodes.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>network</code></strong> :&ensp;<code>SequentialNetwork</code></dt>
<dd>Sequential Network of interest of which we want to combine the nodes.</dd>
</dl>
<h2 id="return">Return</h2>
<p>SequentialNetwork
Corresponding Sequential Network with the combined nodes.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def combine_batchnorm1d_net(network: networks.SequentialNetwork) -&gt; networks.SequentialNetwork:
    &#34;&#34;&#34;
    Utilities function to combine all the FullyConnectedNodes followed by BatchNorm1DNodes in corresponding
    FullyConnectedNodes.

    Parameters
    ----------
    network : SequentialNetwork
        Sequential Network of interest of which we want to combine the nodes.

    Return
    ----------
    SequentialNetwork
        Corresponding Sequential Network with the combined nodes.

    &#34;&#34;&#34;

    if not network.up_to_date:

        for alt_rep in network.alt_rep_cache:

            if alt_rep.up_to_date:

                if isinstance(alt_rep, cv.PyTorchNetwork):
                    pytorch_cv = cv.PyTorchConverter()
                    network = pytorch_cv.to_neural_network(alt_rep)
                elif isinstance(alt_rep, cv.ONNXNetwork):
                    onnx_cv = cv.ONNXConverter
                    network = onnx_cv.to_neural_network(alt_rep)
                else:
                    raise NotImplementedError
                break

    combined_network = networks.SequentialNetwork(network.identifier + &#39;_combined&#39;)

    current_node = network.get_first_node()
    node_index = 1
    while network.get_next_node(current_node) is not None and current_node is not None:

        next_node = network.get_next_node(current_node)
        if isinstance(current_node, nodes.FullyConnectedNode) and isinstance(next_node, nodes.BatchNorm1DNode):
            combined_node = combine_batchnorm1d(current_node, next_node)
            combined_node.identifier = f&#34;Combined_Linear_{node_index}&#34;
            combined_network.add_node(combined_node)
            next_node = network.get_next_node(next_node)

        elif isinstance(current_node, nodes.FullyConnectedNode):
            identifier = f&#34;Linear_{node_index}&#34;
            new_node = nodes.FullyConnectedNode(identifier, current_node.in_features, current_node.out_features,
                                                current_node.weight, current_node.bias)
            combined_network.add_node(new_node)

        elif isinstance(current_node, nodes.ReLUNode):
            identifier = f&#34;ReLU_{node_index}&#34;
            new_node = nodes.ReLUNode(identifier, current_node.num_features)
            combined_network.add_node(new_node)
        else:
            raise NotImplementedError

        node_index += 1
        current_node = next_node

    if isinstance(current_node, nodes.FullyConnectedNode):
        identifier = f&#34;Linear_{node_index}&#34;
        new_node = nodes.FullyConnectedNode(identifier, current_node.in_features, current_node.out_features,
                                            current_node.weight, current_node.bias)
        combined_network.add_node(new_node)
    elif isinstance(current_node, nodes.ReLUNode):
        identifier = f&#34;ReLU_{node_index}&#34;
        new_node = nodes.ReLUNode(identifier, current_node.num_features)
        combined_network.add_node(new_node)
    else:
        raise NotImplementedError

    return combined_network</code></pre>
</details>
</dd>
<dt id="pynever.utilities.generate_targeted_linf_robustness_query"><code class="name flex">
<span>def <span class="ident">generate_targeted_linf_robustness_query</span></span>(<span>data: <a title="pynever.tensor.Tensor" href="tensor.html#pynever.tensor.Tensor">Tensor</a>, adv_target: int, bounds: tuple, num_classes: int, epsilon: float, filepath: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to generate a targeted Robustness SMTLIB query and to save it to a SMTLIB file.
The robustness query is of the kind based on the infinity norm.
It assumes that the data and target are from a classification task.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>Tensor</code></dt>
<dd>Input data of interest.</dd>
<dt><strong><code>adv_target</code></strong> :&ensp;<code>int</code></dt>
<dd>Desired adversarial target for the input data.</dd>
<dt><strong><code>bounds</code></strong> :&ensp;<code>(int, int)</code></dt>
<dd>Bounds for the input data (lower_bound, upper_bound).</dd>
<dt><strong><code>num_classes</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of possible classes.</dd>
<dt><strong><code>epsilon</code></strong> :&ensp;<code>float</code></dt>
<dd>Perturbation with respect to the infinity norm.</dd>
<dt><strong><code>filepath</code></strong> :&ensp;<code>str</code></dt>
<dd>Filepath for the resulting SMTLIB file.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_targeted_linf_robustness_query(data: Tensor, adv_target: int, bounds: tuple,
                                            num_classes: int, epsilon: float, filepath: str):
    &#34;&#34;&#34;
    Function to generate a targeted Robustness SMTLIB query and to save it to a SMTLIB file.
    The robustness query is of the kind based on the infinity norm.
    It assumes that the data and target are from a classification task.

    Parameters
    ----------
    data : Tensor
        Input data of interest.
    adv_target : int
        Desired adversarial target for the input data.
    bounds : (int, int)
        Bounds for the input data (lower_bound, upper_bound).
    num_classes : int
        Number of possible classes.
    epsilon : float
        Perturbation with respect to the infinity norm.
    filepath : str
        Filepath for the resulting SMTLIB file.

    &#34;&#34;&#34;
    with open(filepath, &#34;w&#34;) as f:

        flattened_data = data.flatten()
        for i in range(len(flattened_data)):
            f.write(f&#34;(declare-const X_{i} Real)\n&#34;)

        for i in range(num_classes):
            f.write(f&#34;(declare-const Y_{i} Real)\n&#34;)

        for i in range(len(flattened_data)):

            if flattened_data[i] - epsilon &lt; bounds[0]:
                f.write(f&#34;(assert (&gt;= X_{i} {bounds[0]}))\n&#34;)
            else:
                f.write(f&#34;(assert (&gt;= X_{i} {flattened_data[i] - epsilon}))\n&#34;)

            if flattened_data[i] + epsilon &gt; bounds[1]:
                f.write(f&#34;(assert (&lt;= X_{i} {bounds[1]}))\n&#34;)
            else:
                f.write(f&#34;(assert (&lt;= X_{i} {flattened_data[i] + epsilon}))\n&#34;)

        for i in range(num_classes):

            if i != adv_target:
                f.write(f&#34;(assert (&lt;= (- Y_{i} Y_{adv_target}) 0))\n&#34;)</code></pre>
</details>
</dd>
<dt id="pynever.utilities.generate_untargeted_linf_robustness_query"><code class="name flex">
<span>def <span class="ident">generate_untargeted_linf_robustness_query</span></span>(<span>data: <a title="pynever.tensor.Tensor" href="tensor.html#pynever.tensor.Tensor">Tensor</a>, target: int, bounds: tuple, num_classes: int, epsilon: float, filepath: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to generate an untargeted Robustness SMTLIB query and to save it to a SMTLIB file.
The robustness query is of the kind based on the infinity norm.
It assumes that the data and target are from a classification task.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>Tensor</code></dt>
<dd>Input data of interest.</dd>
<dt><strong><code>adv_target</code></strong> :&ensp;<code>int</code></dt>
<dd>Desired adversarial target for the input data.</dd>
<dt><strong><code>bounds</code></strong> :&ensp;<code>(int, int)</code></dt>
<dd>Bounds for the input data (lower_bound, upper_bound).</dd>
<dt><strong><code>num_classes</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of possible classes.</dd>
<dt><strong><code>epsilon</code></strong> :&ensp;<code>float</code></dt>
<dd>Perturbation with respect to the infinity norm.</dd>
<dt><strong><code>filepath</code></strong> :&ensp;<code>str</code></dt>
<dd>Filepath for the resulting SMTLIB file.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_untargeted_linf_robustness_query(data: Tensor, target: int, bounds: tuple,
                                              num_classes: int, epsilon: float, filepath: str):
    &#34;&#34;&#34;
    Function to generate an untargeted Robustness SMTLIB query and to save it to a SMTLIB file.
    The robustness query is of the kind based on the infinity norm.
    It assumes that the data and target are from a classification task.

    Parameters
    ----------
    data : Tensor
        Input data of interest.
    adv_target : int
        Desired adversarial target for the input data.
    bounds : (int, int)
        Bounds for the input data (lower_bound, upper_bound).
    num_classes : int
        Number of possible classes.
    epsilon : float
        Perturbation with respect to the infinity norm.
    filepath : str
        Filepath for the resulting SMTLIB file.

    &#34;&#34;&#34;
    with open(filepath, &#34;w&#34;) as f:

        flattened_data = data.flatten()
        for i in range(len(flattened_data)):
            f.write(f&#34;(declare-const X_{i} Real)\n&#34;)

        for i in range(num_classes):
            f.write(f&#34;(declare-const Y_{i} Real)\n&#34;)

        for i in range(len(flattened_data)):

            if flattened_data[i] - epsilon &lt; bounds[0]:
                f.write(f&#34;(assert (&gt;= X_{i} {bounds[0]}))\n&#34;)
            else:
                f.write(f&#34;(assert (&gt;= X_{i} {flattened_data[i] - epsilon}))\n&#34;)

            if flattened_data[i] + epsilon &gt; bounds[1]:
                f.write(f&#34;(assert (&lt;= X_{i} {bounds[1]}))\n&#34;)
            else:
                f.write(f&#34;(assert (&lt;= X_{i} {flattened_data[i] + epsilon}))\n&#34;)

        output_query = &#34;(assert (or&#34;
        for i in range(num_classes):

            if i != target:
                output_query += f&#34; (&lt;= (- Y_{target} Y_{i}) 0)&#34;

        output_query += &#34;))&#34;
        f.write(output_query)</code></pre>
</details>
</dd>
<dt id="pynever.utilities.net_update"><code class="name flex">
<span>def <span class="ident">net_update</span></span>(<span>network: <a title="pynever.networks.NeuralNetwork" href="networks.html#pynever.networks.NeuralNetwork">NeuralNetwork</a>) ‑> <a title="pynever.networks.NeuralNetwork" href="networks.html#pynever.networks.NeuralNetwork">NeuralNetwork</a></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def net_update(network: networks.NeuralNetwork) -&gt; networks.NeuralNetwork:

    if not network.up_to_date:

        for alt_rep in network.alt_rep_cache:

            if alt_rep.up_to_date:
                if isinstance(alt_rep, cv.ONNXNetwork):
                    return cv.ONNXConverter().to_neural_network(alt_rep)
                elif isinstance(alt_rep, cv.PyTorchNetwork):
                    return cv.PyTorchConverter().to_neural_network(alt_rep)
                else:
                    raise NotImplementedError

    else:
        return network</code></pre>
</details>
</dd>
<dt id="pynever.utilities.parse_acas_property"><code class="name flex">
<span>def <span class="ident">parse_acas_property</span></span>(<span>filepath: str) ‑> ((<class '<a title="pynever.tensor.Tensor" href="tensor.html#pynever.tensor.Tensor">Tensor</a>'>, <class '<a title="pynever.tensor.Tensor" href="tensor.html#pynever.tensor.Tensor">Tensor</a>'>), (<class '<a title="pynever.tensor.Tensor" href="tensor.html#pynever.tensor.Tensor">Tensor</a>'>, <class '<a title="pynever.tensor.Tensor" href="tensor.html#pynever.tensor.Tensor">Tensor</a>'>))</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def parse_acas_property(filepath: str) -&gt; ((Tensor, Tensor), (Tensor, Tensor)):

    in_coeff = np.zeros((10, 5))
    in_bias = np.zeros((10, 1))
    out_coeff = []
    out_bias = []
    row_index = 0

    with open(filepath, &#39;r&#39;) as f:

        for line in f:

            if line[0] == &#34;x&#34;:
                splitted_line = line.split(&#34; &#34;)
                var_index = int(splitted_line[0][1])
                if splitted_line[1] == &#34;&gt;=&#34;:
                    in_coeff[row_index, var_index] = -1
                    in_bias[row_index] = -float(splitted_line[2])
                else:
                    in_coeff[row_index, var_index] = 1
                    in_bias[row_index] = float(splitted_line[2])

            else:

                splitted_line = line.split(&#34; &#34;)
                if len(splitted_line) == 3:
                    var_index = int(splitted_line[0][1])
                    temp = np.zeros(5)
                    if splitted_line[1] == &#34;&gt;=&#34;:
                        temp[var_index] = -1
                        out_coeff.append(temp)
                        out_bias.append(-float(splitted_line[2]))
                    else:
                        temp[var_index] = 1
                        out_coeff.append(temp)
                        out_bias.append(float(splitted_line[2]))
                else:
                    var_index_1 = int(splitted_line[0][2])
                    var_index_2 = int(splitted_line[1][2])
                    temp = np.zeros(5)
                    if splitted_line[2] == &#34;&gt;=&#34;:
                        temp[var_index_1] = -1
                        temp[var_index_2] = 1
                        out_coeff.append(temp)
                        out_bias.append(-float(splitted_line[3]))
                    else:
                        temp[var_index_1] = 1
                        temp[var_index_2] = -1
                        out_coeff.append(temp)
                        out_bias.append(float(splitted_line[3]))

            row_index = row_index + 1

        out_coeff = np.array(out_coeff)
        array_out_bias = np.zeros((len(out_bias), 1))

        for i in range(len(out_bias)):
            array_out_bias[i, 0] = out_bias[i]

        out_bias = array_out_bias

    return (in_coeff, in_bias), (out_coeff, out_bias)</code></pre>
</details>
</dd>
<dt id="pynever.utilities.parse_linf_robustness_smtlib"><code class="name flex">
<span>def <span class="ident">parse_linf_robustness_smtlib</span></span>(<span>filepath: str) ‑> (<class 'bool'>, <class 'list'>, <class 'int'>)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to extract the parameters of a robustness query from the smtlib file.
It assume the SMTLIB file is structured as following:</p>
<pre><code>; definition of the variables of interest
(declare-const X_0 Real)
(declare-const X_1 Real)
...
(declare-const Y_1 Real)
(declare-const Y_2 Real)
...
; definition of the constraints
(assert (&gt;= X_0 eps_0))
(assert (&lt;= X_0 eps_1))
...
(assert (&lt;= (- Y_0 Y_1) 0))
...
</code></pre>
<p>Where the eps_i are Real numbers.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filepath</code></strong> :&ensp;<code>str</code></dt>
<dd>Filepath to the SMTLIB file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(bool, list, int)
Tuple of list: the first list contains the values eps_i for each variables as tuples (lower_bound, upper_bound),
while the int correspond to the desired target for the related data.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def parse_linf_robustness_smtlib(filepath: str) -&gt; (bool, list, int):
    &#34;&#34;&#34;
    Function to extract the parameters of a robustness query from the smtlib file.
    It assume the SMTLIB file is structured as following:

        ; definition of the variables of interest
        (declare-const X_0 Real)
        (declare-const X_1 Real)
        ...
        (declare-const Y_1 Real)
        (declare-const Y_2 Real)
        ...
        ; definition of the constraints
        (assert (&gt;= X_0 eps_0))
        (assert (&lt;= X_0 eps_1))
        ...
        (assert (&lt;= (- Y_0 Y_1) 0))
        ...

    Where the eps_i are Real numbers.

    Parameters
    ----------
    filepath : str
        Filepath to the SMTLIB file.

    Returns
    ----------
    (bool, list, int)
        Tuple of list: the first list contains the values eps_i for each variables as tuples (lower_bound, upper_bound),
        while the int correspond to the desired target for the related data.
    &#34;&#34;&#34;
    targeted = True
    correct_target = -1
    lb = []
    ub = []
    with open(filepath, &#39;r&#39;) as f:

        for line in f:

            line = line.replace(&#39;(&#39;, &#39;( &#39;)
            line = line.replace(&#39;)&#39;, &#39; )&#39;)
            if line[0] == &#39;(&#39;:
                aux = line.split()
                if aux[1] == &#39;assert&#39;:

                    if aux[4] == &#39;(&#39;:
                        if aux[3] == &#39;or&#39;:
                            targeted = False
                            temp = aux[8].split(&#34;_&#34;)
                            correct_target = int(temp[1])
                        else:
                            targeted = True
                            temp = aux[7].split(&#34;_&#34;)
                            correct_target = int(temp[1])

                    else:

                        if aux[3] == &#34;&gt;=&#34;:
                            lb.append(float(aux[5]))
                        else:
                            ub.append(float(aux[5]))

    input_bounds = []
    for i in range(len(lb)):
        input_bounds.append((lb[i], ub[i]))

    return targeted, input_bounds, correct_target</code></pre>
</details>
</dd>
<dt id="pynever.utilities.parse_nnet"><code class="name flex">
<span>def <span class="ident">parse_nnet</span></span>(<span>filepath: str) ‑> (<class 'list'>, <class 'list'>, <class 'list'>, <class 'list'>, <class 'list'>, <class 'list'>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def parse_nnet(filepath: str) -&gt; (list, list, list, list, list, list):

    with open(filepath) as f:

        line = f.readline()
        cnt = 1
        while line[0:2] == &#34;//&#34;:
            line = f.readline()
            cnt += 1
        # numLayers does&#39;t include the input layer!
        numLayers, inputSize, outputSize, maxLayersize = [int(x) for x in line.strip().split(&#34;,&#34;)[:-1]]
        line = f.readline()

        # input layer size, layer1size, layer2size...
        layerSizes = [int(x) for x in line.strip().split(&#34;,&#34;)[:-1]]

        line = f.readline()
        symmetric = int(line.strip().split(&#34;,&#34;)[0])

        line = f.readline()
        inputMinimums = [float(x) for x in line.strip().split(&#34;,&#34;)[:-1]]

        line = f.readline()
        inputMaximums = [float(x) for x in line.strip().split(&#34;,&#34;)[:-1]]

        line = f.readline()
        means = [float(x) for x in line.strip().split(&#34;,&#34;)[:-1]]

        line = f.readline()
        ranges = [float(x) for x in line.strip().split(&#34;,&#34;)[:-1]]

        weights = []
        biases = []
        for layernum in range(numLayers):

            previousLayerSize = layerSizes[layernum]
            currentLayerSize = layerSizes[layernum + 1]
            # weights
            weights.append([])
            biases.append([])
            # weights
            for i in range(currentLayerSize):
                line = f.readline()
                aux = [float(x) for x in line.strip().split(&#34;,&#34;)[:-1]]
                weights[layernum].append([])
                for j in range(previousLayerSize):
                    weights[layernum][i].append(aux[j])
            # biases
            for i in range(currentLayerSize):
                line = f.readline()
                x = float(line.strip().split(&#34;,&#34;)[0])
                biases[layernum].append(x)

        numLayers = numLayers
        layerSizes = layerSizes
        inputSize = inputSize
        outputSize = outputSize
        maxLayersize = maxLayersize
        inputMinimums = inputMinimums
        inputMaximums = inputMaximums
        inputMeans = means[:-1]
        inputRanges = ranges[:-1]
        outputMean = means[-1]
        outputRange = ranges[-1]
        weights = weights
        biases = biases

        new_weights = []
        new_biases = []
        for i in range(numLayers):
            weight = np.array(weights[i])
            bias = np.array(biases[i])

            new_weights.append(weight)
            new_biases.append(bias)

        return new_weights, new_biases, inputMeans, inputRanges, outputMean, outputRange</code></pre>
</details>
</dd>
<dt id="pynever.utilities.testing"><code class="name flex">
<span>def <span class="ident">testing</span></span>(<span>net: <a title="pynever.strategies.conversion.PyTorchNetwork" href="strategies/conversion.html#pynever.strategies.conversion.PyTorchNetwork">PyTorchNetwork</a>, dataset: <a title="pynever.datasets.Dataset" href="datasets.html#pynever.datasets.Dataset">Dataset</a>, test_batch_size: int, cuda: bool) ‑> (<class 'float'>, <class 'float'>)</span>
</code></dt>
<dd>
<div class="desc"><p>Testing procedure for a PyTorchNetwork.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>net</code></strong> :&ensp;<code>PyTorchNetwork</code></dt>
<dd>Neural Network to test.</dd>
<dt><strong><code>dataset</code></strong> :&ensp;<code>Dataset</code></dt>
<dd>Dataset used for testing the network.</dd>
<dt><strong><code>test_batch_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Dimension for the test batch size for the testing procedure</dd>
<dt><strong><code>cuda</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to use the cuda library for the procedure (default: False).</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(float, float)
Rate of correct samples and loss.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def testing(net: cv.PyTorchNetwork, dataset: datasets.Dataset, test_batch_size: int, cuda: bool) -&gt; (float, float):
    &#34;&#34;&#34;
    Testing procedure for a PyTorchNetwork.

    Parameters
    ----------
    net : PyTorchNetwork
        Neural Network to test.
    dataset : Dataset
        Dataset used for testing the network.
    test_batch_size : int
        Dimension for the test batch size for the testing procedure
    cuda : bool
        Whether to use the cuda library for the procedure (default: False).

    Returns
    ----------
    (float, float)
        Rate of correct samples and loss.

    &#34;&#34;&#34;

    if cuda:
        net.pytorch_network.cuda()
    else:
        net.pytorch_network.cpu()

    test_set = dataset.get_test_set()

    net.pytorch_network.eval()
    net.pytorch_network.float()
    test_loss = 0
    correct = 0
    with torch.no_grad():

        batch_idx = 0
        data_idx = 0

        while data_idx &lt; len(test_set[0]):

            if data_idx + test_batch_size &gt;= len(test_set[0]):
                last_data_idx = len(test_set[0])
            else:
                last_data_idx = data_idx + test_batch_size

            data = torch.from_numpy(test_set[0][data_idx:last_data_idx, :])
            target = torch.from_numpy(test_set[1][data_idx:last_data_idx])

            if cuda:
                data, target = data.cuda(), target.cuda()

            data, target = torch.autograd.Variable(data), torch.autograd.Variable(target)
            output = net.pytorch_network(data)
            test_loss += funct.cross_entropy(output, target, reduction=&#39;sum&#39;).data.item()  # sum up batch loss
            pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability
            correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()
            batch_idx += 1
            data_idx += test_batch_size

    test_loss /= float(math.floor(len(test_set[0]) / test_batch_size))
    print(&#39;\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.1f}%)\n&#39;.format(
        test_loss, correct, len(test_set[0]),
        100. * correct / len(test_set[0])))

    return correct / float(len(test_set[0])), test_loss</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pynever" href="index.html">pynever</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="pynever.utilities.combine_batchnorm1d" href="#pynever.utilities.combine_batchnorm1d">combine_batchnorm1d</a></code></li>
<li><code><a title="pynever.utilities.combine_batchnorm1d_net" href="#pynever.utilities.combine_batchnorm1d_net">combine_batchnorm1d_net</a></code></li>
<li><code><a title="pynever.utilities.generate_targeted_linf_robustness_query" href="#pynever.utilities.generate_targeted_linf_robustness_query">generate_targeted_linf_robustness_query</a></code></li>
<li><code><a title="pynever.utilities.generate_untargeted_linf_robustness_query" href="#pynever.utilities.generate_untargeted_linf_robustness_query">generate_untargeted_linf_robustness_query</a></code></li>
<li><code><a title="pynever.utilities.net_update" href="#pynever.utilities.net_update">net_update</a></code></li>
<li><code><a title="pynever.utilities.parse_acas_property" href="#pynever.utilities.parse_acas_property">parse_acas_property</a></code></li>
<li><code><a title="pynever.utilities.parse_linf_robustness_smtlib" href="#pynever.utilities.parse_linf_robustness_smtlib">parse_linf_robustness_smtlib</a></code></li>
<li><code><a title="pynever.utilities.parse_nnet" href="#pynever.utilities.parse_nnet">parse_nnet</a></code></li>
<li><code><a title="pynever.utilities.testing" href="#pynever.utilities.testing">testing</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>