<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>pynever.nodes API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pynever.nodes</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import abc
from pynever.tensor import Tensor
import numpy as np


class LayerNode(abc.ABC):
    &#34;&#34;&#34;
    An abstract class used for our internal representation of a generic Layer of a Neural Network.
    Its concrete children correspond to real network layers.

    Attributes
    ----------
    identifier : str
        Identifier of the LayerNode.

    &#34;&#34;&#34;

    def __init__(self, identifier: str):

        self.identifier = identifier


class ReLUNode(LayerNode):
    &#34;&#34;&#34;
    A class used for our internal representation of a ReLU Layer of a Neural Network.

    Attributes
    ----------
    identifier : str
        Identifier of the LayerNode.
    num_features : int
        Number of input and output features of the ReLU layer.
    &#34;&#34;&#34;

    def __init__(self, identifier: str, num_features):
        super().__init__(identifier)
        self.num_features = num_features

    def __repr__(self):

        return f&#34;ReLUNode - num_features = {self.num_features}&#34;

    def to_string(self):

        return f&#34;ReLUNode - num_features = {self.num_features}&#34;


class SigmoidNode(LayerNode):
    &#34;&#34;&#34;
    A class used for our internal representation of a Sigmoid Layer of a Neural Network.
    TO DO: It is not supported in the existing conversion strategies.

    Attributes
    ----------
    identifier : str
        Identifier of the LayerNode.
    num_features : int
        Number of input and output features of the Sigmoid layer.

    &#34;&#34;&#34;

    def __init__(self, identifier: str, num_features):
        super().__init__(identifier)
        self.num_features = num_features

    def __repr__(self):

        return f&#34;SigmoidNode - num_features = {self.num_features}&#34;

    def to_string(self):

        return f&#34;SigmoidNode - num_features = {self.num_features}&#34;


class FullyConnectedNode(LayerNode):
    &#34;&#34;&#34;
    A class used for our internal representation of a Fully Connected layer of a Neural Network

    Attributes
    ----------
    identifier : str
        Identifier of the LayerNode.
    in_features : int
        Number of input features of the fully connected layer.
    out_features : int
        Number of output features of the fully connected layer.
    weight : Tensor, optional
        Tensor containing the weight parameters of the fully connected layer.
    bias : Tensor, optional
        Tensor containing the bias parameters of the fully connected layer.
    &#34;&#34;&#34;

    def __init__(self, identifier: str, in_features: int, out_features: int,
                 weight: Tensor = None, bias: Tensor = None):

        super().__init__(identifier)
        self.in_features = in_features
        self.out_features = out_features

        if weight is None:
            weight = np.random.normal(size=[out_features, in_features])

        if bias is None:
            bias = np.random.normal(size=[out_features])

        self.weight = weight
        self.bias = bias

    def __repr__(self):

        return f&#34;FullyConnectedNode - in_features = {self.in_features}, out_features = {self.out_features}&#34;

    def to_string(self):

        return f&#34;FullyConnectedNode - in_features = {self.in_features}, out_features = {self.out_features}&#34;


class BatchNorm1DNode(LayerNode):
    &#34;&#34;&#34;
    A class used for our internal representation of a one dimensional Batch Normalization Layer.

    Attributes
    ----------
    identifier : str
        Identifier of the LayerNode.
    num_features : int
        Number of input and output feature of the Batch Normalization Layer.
    weight : Tensor, optional
        Tensor containing the weight parameters of the Batch Normalization Layer.
    bias : Tensor, optional
        Tensor containing the bias parameter of the Batch Normalization Layer.
    running_mean : Tensor, optional
        Tensor containing the running mean parameter of the Batch Normalization Layer.
    running_var : Tensor, optional
        Tensor containing the running var parameter of the Batch Normalization Layer.
    eps : float, optional
        Value added to the denominator for numerical stability (default: 1e-5).
    momentum : float, optional
        Value used for the running_mean and running_var computation. Can be set to None
        for cumulative moving average (default: 0.1)
    affine : bool, optional
        When set to True, the module has learnable affine parameter (default: True).
    track_running_stats : bool, optional
        When set to True, the module tracks the running mean and variance, when set to false the module
        does not track such statistics and always uses batch statistics in both training and eval modes (default: True).

    &#34;&#34;&#34;

    def __init__(self, identifier: str, num_features: int, weight: Tensor = None, bias: Tensor = None,
                 running_mean: Tensor = None, running_var: Tensor = None, eps: float = 1e-5, momentum: float = 0.1,
                 affine: bool = True, track_running_stats: bool = True):

        super().__init__(identifier)
        self.num_features = num_features

        if track_running_stats and running_mean is None and running_var is None:
            running_mean = np.ones(num_features)
            running_var = np.zeros(num_features)

        if weight is None:
            weight = np.ones(num_features)

        if bias is None:
            bias = np.zeros(num_features)

        self.weight = weight
        self.bias = bias
        self.running_mean = running_mean
        self.running_var = running_var

        self.eps = eps
        self.momentum = momentum
        self.affine = affine
        self.track_running_stats = track_running_stats

    def __repr__(self):

        return f&#34;BatchNorm1DNode - num_features = {self.num_features}&#34;

    def to_string(self):
        return f&#34;BatchNorm1DNode - num_features = {self.num_features}&#34;</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="pynever.nodes.BatchNorm1DNode"><code class="flex name class">
<span>class <span class="ident">BatchNorm1DNode</span></span>
<span>(</span><span>identifier: str, num_features: int, weight: <a title="pynever.tensor.Tensor" href="tensor.html#pynever.tensor.Tensor">Tensor</a> = None, bias: <a title="pynever.tensor.Tensor" href="tensor.html#pynever.tensor.Tensor">Tensor</a> = None, running_mean: <a title="pynever.tensor.Tensor" href="tensor.html#pynever.tensor.Tensor">Tensor</a> = None, running_var: <a title="pynever.tensor.Tensor" href="tensor.html#pynever.tensor.Tensor">Tensor</a> = None, eps: float = 1e-05, momentum: float = 0.1, affine: bool = True, track_running_stats: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>A class used for our internal representation of a one dimensional Batch Normalization Layer.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>identifier</code></strong> :&ensp;<code>str</code></dt>
<dd>Identifier of the LayerNode.</dd>
<dt><strong><code>num_features</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of input and output feature of the Batch Normalization Layer.</dd>
<dt><strong><code>weight</code></strong> :&ensp;<code>Tensor</code>, optional</dt>
<dd>Tensor containing the weight parameters of the Batch Normalization Layer.</dd>
<dt><strong><code>bias</code></strong> :&ensp;<code>Tensor</code>, optional</dt>
<dd>Tensor containing the bias parameter of the Batch Normalization Layer.</dd>
<dt><strong><code>running_mean</code></strong> :&ensp;<code>Tensor</code>, optional</dt>
<dd>Tensor containing the running mean parameter of the Batch Normalization Layer.</dd>
<dt><strong><code>running_var</code></strong> :&ensp;<code>Tensor</code>, optional</dt>
<dd>Tensor containing the running var parameter of the Batch Normalization Layer.</dd>
<dt><strong><code>eps</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Value added to the denominator for numerical stability (default: 1e-5).</dd>
<dt><strong><code>momentum</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Value used for the running_mean and running_var computation. Can be set to None
for cumulative moving average (default: 0.1)</dd>
<dt><strong><code>affine</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>When set to True, the module has learnable affine parameter (default: True).</dd>
<dt><strong><code>track_running_stats</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>When set to True, the module tracks the running mean and variance, when set to false the module
does not track such statistics and always uses batch statistics in both training and eval modes (default: True).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BatchNorm1DNode(LayerNode):
    &#34;&#34;&#34;
    A class used for our internal representation of a one dimensional Batch Normalization Layer.

    Attributes
    ----------
    identifier : str
        Identifier of the LayerNode.
    num_features : int
        Number of input and output feature of the Batch Normalization Layer.
    weight : Tensor, optional
        Tensor containing the weight parameters of the Batch Normalization Layer.
    bias : Tensor, optional
        Tensor containing the bias parameter of the Batch Normalization Layer.
    running_mean : Tensor, optional
        Tensor containing the running mean parameter of the Batch Normalization Layer.
    running_var : Tensor, optional
        Tensor containing the running var parameter of the Batch Normalization Layer.
    eps : float, optional
        Value added to the denominator for numerical stability (default: 1e-5).
    momentum : float, optional
        Value used for the running_mean and running_var computation. Can be set to None
        for cumulative moving average (default: 0.1)
    affine : bool, optional
        When set to True, the module has learnable affine parameter (default: True).
    track_running_stats : bool, optional
        When set to True, the module tracks the running mean and variance, when set to false the module
        does not track such statistics and always uses batch statistics in both training and eval modes (default: True).

    &#34;&#34;&#34;

    def __init__(self, identifier: str, num_features: int, weight: Tensor = None, bias: Tensor = None,
                 running_mean: Tensor = None, running_var: Tensor = None, eps: float = 1e-5, momentum: float = 0.1,
                 affine: bool = True, track_running_stats: bool = True):

        super().__init__(identifier)
        self.num_features = num_features

        if track_running_stats and running_mean is None and running_var is None:
            running_mean = np.ones(num_features)
            running_var = np.zeros(num_features)

        if weight is None:
            weight = np.ones(num_features)

        if bias is None:
            bias = np.zeros(num_features)

        self.weight = weight
        self.bias = bias
        self.running_mean = running_mean
        self.running_var = running_var

        self.eps = eps
        self.momentum = momentum
        self.affine = affine
        self.track_running_stats = track_running_stats

    def __repr__(self):

        return f&#34;BatchNorm1DNode - num_features = {self.num_features}&#34;

    def to_string(self):
        return f&#34;BatchNorm1DNode - num_features = {self.num_features}&#34;</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="pynever.nodes.LayerNode" href="#pynever.nodes.LayerNode">LayerNode</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="pynever.nodes.BatchNorm1DNode.to_string"><code class="name flex">
<span>def <span class="ident">to_string</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_string(self):
    return f&#34;BatchNorm1DNode - num_features = {self.num_features}&#34;</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pynever.nodes.FullyConnectedNode"><code class="flex name class">
<span>class <span class="ident">FullyConnectedNode</span></span>
<span>(</span><span>identifier: str, in_features: int, out_features: int, weight: <a title="pynever.tensor.Tensor" href="tensor.html#pynever.tensor.Tensor">Tensor</a> = None, bias: <a title="pynever.tensor.Tensor" href="tensor.html#pynever.tensor.Tensor">Tensor</a> = None)</span>
</code></dt>
<dd>
<div class="desc"><p>A class used for our internal representation of a Fully Connected layer of a Neural Network</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>identifier</code></strong> :&ensp;<code>str</code></dt>
<dd>Identifier of the LayerNode.</dd>
<dt><strong><code>in_features</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of input features of the fully connected layer.</dd>
<dt><strong><code>out_features</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of output features of the fully connected layer.</dd>
<dt><strong><code>weight</code></strong> :&ensp;<code>Tensor</code>, optional</dt>
<dd>Tensor containing the weight parameters of the fully connected layer.</dd>
<dt><strong><code>bias</code></strong> :&ensp;<code>Tensor</code>, optional</dt>
<dd>Tensor containing the bias parameters of the fully connected layer.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FullyConnectedNode(LayerNode):
    &#34;&#34;&#34;
    A class used for our internal representation of a Fully Connected layer of a Neural Network

    Attributes
    ----------
    identifier : str
        Identifier of the LayerNode.
    in_features : int
        Number of input features of the fully connected layer.
    out_features : int
        Number of output features of the fully connected layer.
    weight : Tensor, optional
        Tensor containing the weight parameters of the fully connected layer.
    bias : Tensor, optional
        Tensor containing the bias parameters of the fully connected layer.
    &#34;&#34;&#34;

    def __init__(self, identifier: str, in_features: int, out_features: int,
                 weight: Tensor = None, bias: Tensor = None):

        super().__init__(identifier)
        self.in_features = in_features
        self.out_features = out_features

        if weight is None:
            weight = np.random.normal(size=[out_features, in_features])

        if bias is None:
            bias = np.random.normal(size=[out_features])

        self.weight = weight
        self.bias = bias

    def __repr__(self):

        return f&#34;FullyConnectedNode - in_features = {self.in_features}, out_features = {self.out_features}&#34;

    def to_string(self):

        return f&#34;FullyConnectedNode - in_features = {self.in_features}, out_features = {self.out_features}&#34;</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="pynever.nodes.LayerNode" href="#pynever.nodes.LayerNode">LayerNode</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="pynever.nodes.FullyConnectedNode.to_string"><code class="name flex">
<span>def <span class="ident">to_string</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_string(self):

    return f&#34;FullyConnectedNode - in_features = {self.in_features}, out_features = {self.out_features}&#34;</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pynever.nodes.LayerNode"><code class="flex name class">
<span>class <span class="ident">LayerNode</span></span>
<span>(</span><span>identifier: str)</span>
</code></dt>
<dd>
<div class="desc"><p>An abstract class used for our internal representation of a generic Layer of a Neural Network.
Its concrete children correspond to real network layers.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>identifier</code></strong> :&ensp;<code>str</code></dt>
<dd>Identifier of the LayerNode.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LayerNode(abc.ABC):
    &#34;&#34;&#34;
    An abstract class used for our internal representation of a generic Layer of a Neural Network.
    Its concrete children correspond to real network layers.

    Attributes
    ----------
    identifier : str
        Identifier of the LayerNode.

    &#34;&#34;&#34;

    def __init__(self, identifier: str):

        self.identifier = identifier</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="pynever.nodes.BatchNorm1DNode" href="#pynever.nodes.BatchNorm1DNode">BatchNorm1DNode</a></li>
<li><a title="pynever.nodes.FullyConnectedNode" href="#pynever.nodes.FullyConnectedNode">FullyConnectedNode</a></li>
<li><a title="pynever.nodes.ReLUNode" href="#pynever.nodes.ReLUNode">ReLUNode</a></li>
<li><a title="pynever.nodes.SigmoidNode" href="#pynever.nodes.SigmoidNode">SigmoidNode</a></li>
</ul>
</dd>
<dt id="pynever.nodes.ReLUNode"><code class="flex name class">
<span>class <span class="ident">ReLUNode</span></span>
<span>(</span><span>identifier: str, num_features)</span>
</code></dt>
<dd>
<div class="desc"><p>A class used for our internal representation of a ReLU Layer of a Neural Network.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>identifier</code></strong> :&ensp;<code>str</code></dt>
<dd>Identifier of the LayerNode.</dd>
<dt><strong><code>num_features</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of input and output features of the ReLU layer.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ReLUNode(LayerNode):
    &#34;&#34;&#34;
    A class used for our internal representation of a ReLU Layer of a Neural Network.

    Attributes
    ----------
    identifier : str
        Identifier of the LayerNode.
    num_features : int
        Number of input and output features of the ReLU layer.
    &#34;&#34;&#34;

    def __init__(self, identifier: str, num_features):
        super().__init__(identifier)
        self.num_features = num_features

    def __repr__(self):

        return f&#34;ReLUNode - num_features = {self.num_features}&#34;

    def to_string(self):

        return f&#34;ReLUNode - num_features = {self.num_features}&#34;</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="pynever.nodes.LayerNode" href="#pynever.nodes.LayerNode">LayerNode</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="pynever.nodes.ReLUNode.to_string"><code class="name flex">
<span>def <span class="ident">to_string</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_string(self):

    return f&#34;ReLUNode - num_features = {self.num_features}&#34;</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pynever.nodes.SigmoidNode"><code class="flex name class">
<span>class <span class="ident">SigmoidNode</span></span>
<span>(</span><span>identifier: str, num_features)</span>
</code></dt>
<dd>
<div class="desc"><p>A class used for our internal representation of a Sigmoid Layer of a Neural Network.
TO DO: It is not supported in the existing conversion strategies.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>identifier</code></strong> :&ensp;<code>str</code></dt>
<dd>Identifier of the LayerNode.</dd>
<dt><strong><code>num_features</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of input and output features of the Sigmoid layer.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SigmoidNode(LayerNode):
    &#34;&#34;&#34;
    A class used for our internal representation of a Sigmoid Layer of a Neural Network.
    TO DO: It is not supported in the existing conversion strategies.

    Attributes
    ----------
    identifier : str
        Identifier of the LayerNode.
    num_features : int
        Number of input and output features of the Sigmoid layer.

    &#34;&#34;&#34;

    def __init__(self, identifier: str, num_features):
        super().__init__(identifier)
        self.num_features = num_features

    def __repr__(self):

        return f&#34;SigmoidNode - num_features = {self.num_features}&#34;

    def to_string(self):

        return f&#34;SigmoidNode - num_features = {self.num_features}&#34;</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="pynever.nodes.LayerNode" href="#pynever.nodes.LayerNode">LayerNode</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="pynever.nodes.SigmoidNode.to_string"><code class="name flex">
<span>def <span class="ident">to_string</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_string(self):

    return f&#34;SigmoidNode - num_features = {self.num_features}&#34;</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pynever" href="index.html">pynever</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="pynever.nodes.BatchNorm1DNode" href="#pynever.nodes.BatchNorm1DNode">BatchNorm1DNode</a></code></h4>
<ul class="">
<li><code><a title="pynever.nodes.BatchNorm1DNode.to_string" href="#pynever.nodes.BatchNorm1DNode.to_string">to_string</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pynever.nodes.FullyConnectedNode" href="#pynever.nodes.FullyConnectedNode">FullyConnectedNode</a></code></h4>
<ul class="">
<li><code><a title="pynever.nodes.FullyConnectedNode.to_string" href="#pynever.nodes.FullyConnectedNode.to_string">to_string</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pynever.nodes.LayerNode" href="#pynever.nodes.LayerNode">LayerNode</a></code></h4>
</li>
<li>
<h4><code><a title="pynever.nodes.ReLUNode" href="#pynever.nodes.ReLUNode">ReLUNode</a></code></h4>
<ul class="">
<li><code><a title="pynever.nodes.ReLUNode.to_string" href="#pynever.nodes.ReLUNode.to_string">to_string</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pynever.nodes.SigmoidNode" href="#pynever.nodes.SigmoidNode">SigmoidNode</a></code></h4>
<ul class="">
<li><code><a title="pynever.nodes.SigmoidNode.to_string" href="#pynever.nodes.SigmoidNode.to_string">to_string</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>